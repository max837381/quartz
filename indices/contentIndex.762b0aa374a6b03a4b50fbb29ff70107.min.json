{"/":{"title":"📓 Max’s College Notes","content":"\nA collection of my notes starting from my Master’s degree at UCSC.\n\n[[notes/Macroeconomics]]\n\n[[notes/Taxation]]\n\n[[notes/Causal Inference]]\n\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/2SLS":{"title":"2SLS","content":"\n2SLS stands for two-stage least squares and is a statistical method used in econometrics to estimate the causal relationship between two variables, where one variable affects the other. \n\nIn the first stage of 2SLS, instrumental variables are used to estimate the endogenous variable, which is affected by other variables in the model. These instrumental variables are exogenous (not affected by any other variable), highly correlated with the endogenous variable, and have a direct effect on the explanatory variable. \n\nIn the second stage of 2SLS, the estimated endogenous variable is used as an independent variable in a regression model to estimate its effect on the dependent variable.\n\nFor example, let's say we want to estimate the effect of education on wages. Education can be considered an endogenous variable because it may be affected by other variables such as parental income or ability level. To address this endogeneity issue, we can use an instrumental variable such as proximity to a college or university. We use this instrumental variable to predict education levels and then use education levels as an independent variable in our regression model to estimate its effect on wages.\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/AAAA-Test":{"title":"AAAA Test","content":"\n\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/Actuarial-Value":{"title":"Actuarial Value","content":"\nA fair market value for a risky payoff - typically the [Expected Value](Expected%20Value.md)\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/Adverse-Selection":{"title":"Adverse Selection","content":"\nDefine adverse selection and explain in terms of economics\n\nAdverse selection is a phenomenon that occurs when one party in a transaction has more information than the other party and uses this information to their advantage, which can result in negative consequences for the other party. In economics, adverse selection is often associated with insurance markets, where individuals who are more likely to make claims (i.e., those with higher risks) are more likely to purchase insurance coverage than those with lower risks. This can lead to an imbalance in the insurance pool, resulting in higher premiums for everyone and potentially causing insurers to exit the market altogether. Adverse selection can also occur in labor markets, where employers may be hesitant to hire workers who have a history of health problems or high turnover rates, leading to unequal opportunities and discrimination. Overall, adverse selection can be detrimental to market efficiency and fairness, which is why policymakers often look for ways to mitigate its effects through regulations or other interventions.\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/Aleatoric":{"title":"Aleatoric","content":"\nInvolving games of chance\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/Applied-Economics":{"title":"Applied Economics","content":"\n## Interests\n\n[Causal Inference](Causal%20Inference.md)\n\n[Counterfactual Prediction](Counterfactual%20Prediction.md)\n\nThis branch of economics is now intersecting with [Machine Learning](Machine%20Learning.md)\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/Asymptotics":{"title":"Asymptotics","content":"\nThe behavior of a function $\\large f(x)$ as $\\large x$ becomes very large $\\large x \\rightarrow \\infty$\n\nAsymptotics in statistics refers to the behavior of statistical estimators, tests, and other measures as the sample size or other relevant parameters grow to infinity. Asymptotic theory provides valuable insights into the properties of statistical methods, such as their efficiency, consistency, and robustness.\n\nOne reason why we care about asymptotics in statistics is that it helps us understand the behavior of statistical procedures in large samples, which are often used in practical applications. Asymptotic results provide us with guidance on how to choose and evaluate statistical methods that are reliable and efficient, even as the sample size grows to infinity.\n\nAnother reason why we care about asymptotics is that it provides a theoretical basis for evaluating the performance of statistical methods, even when the sample size is finite. Asymptotic results can give us a sense of how well a statistical method is likely to perform in practice, which can help us select appropriate methods and interpret the results of our analyses.\n\nOverall, asymptotic theory is an important tool for understanding the behavior and properties of statistical methods, and is essential for the development of reliable and efficient statistical procedures.\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/Auto-correlation":{"title":"Auto correlation","content":"\ne_t = rho_1\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/Averages":{"title":"Averages","content":"\n## Unweighted Average\n\nAlso known as the arithmetic mean\n\n## Weighted Average\n\n## When to use either:\n\nYes, choosing the appropriate approach for calculating the average depends on the context and purpose of the analysis. Here are a few examples:\n\n1. Market Research: Suppose you are conducting market research to find out the average income of people in a certain region. In this case, an unweighted average would be appropriate because each person's income should be given the same weight. You want to know the average income of all the people in the region, not just the wealthy individuals who earn more. Therefore, you would use an unweighted average to calculate the average income of the region.\n\n1. Academic Grades: Suppose you want to calculate the average grade of a class, but you also want to take into account the number of credits each course carries. In this case, a weighted average would be appropriate because the number of credits each course carries is a measure of its importance in the calculation of the overall grade. So, a course that carries more credits should be given more weight in the calculation of the average grade. Therefore, you would use a weighted average to calculate the average grade of the class.\n\n1. Environmental Sampling: Suppose you take soil samples from several different locations in a park to measure the concentration of a particular pollutant. However, some locations have more soil than others. In this case, a weighted average would be appropriate because the amount of soil at each location affects the overall concentration of the pollutant. A location with more soil will contribute more to the overall concentration than a location with less soil. Therefore, you would use a weighted average to calculate the overall concentration of the pollutant in the park.\n\nIn summary, the choice between weighted and unweighted averages depends on the context and purpose of the analysis. If all values are equally important, use an unweighted average. If certain values are more important than others, use a weighted average, where the weights reflect the relative importance of each value.\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/Background-Information":{"title":"Background Information","content":"\nIn [Bayesian Statistics](Bayesian%20Statistics.md), the significance of background information, also known as prior knowledge, is crucial in determining the [Probability](Probability.md) of a [Hypothesis](Hypothesis.md) or model given new data.\n\nIn Bayesian inference, the *Prior Probability Distribution* represents what we know about the probability of the hypothesis or model before observing any data. \n\nThis prior information can come from previous experiments, domain knowledge, expert opinions, or other sources of information.\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/Bayes-Box":{"title":"Bayes Box","content":"\nBayes Box is a tool for visualizing the relationships between *Probability Distribution* and events. It is based on [Bayes Theorem](Bayes%20Theorem.md), which states that the [Probability](Probability.md) of an event, given some prior knowledge, can be calculated by multiplying the [prior probability](Prior%20Probability.md) of the event by the probability of the event given the prior knowledge. This is expressed using the following formula:\n\n$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$\n\nWhere $P(A|B)$ is the probability of event A occurring given that event B has already occurred, $P(B|A)$ is the probability of event B occurring given that event A has already occurred, $P(A)$ is the probability of event A occurring, and $P(B)$ is the probability of event B occurring. Bayes Box takes this formula and visualizes it, giving us a graphical representation of the relationships between probability distributions and events. It can be used to calculate the probability of certain events, given prior knowledge, and can also be used to make predictions about future events.\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/Bayes-Rule":{"title":"Bayes Rule","content":"\nBayes' Rule is a theorem in probability theory that states that the probability of an event, given some prior knowledge, can be calculated by multiplying the prior probability of the event by the probability of the event given the prior knowledge. This is expressed using the following formula:\n\n$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$\n\nWhere $P(A|B)$ is the probability of event A occurring given that event B has already occurred, $P(B|A)$ is the probability of event B occurring given that event A has already occurred, $P(A)$ is the probability of event A occurring, and $P(B)$ is the probability of event B occurring. This theorem is often referred to as Bayes' Theorem or Bayes' Rule. In the context of Bayes' Box, Bayes' Rule is used to calculate the probability of a certain event occurring, given some prior information.\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/Bayes-Theorem":{"title":"Bayes Theorem","content":"\n---\n\nWhat is Bayes Theorem?\n\nBayes Theorem is a way to figure out how likely something is to be true. It helps you make an educated guess about something that might be true, even if you don't know for sure.\n\nImagine you have a bowl full of jelly beans, and you can't see what color each jelly bean is. You can make an educated guess, though, based on how many of each color you've seen before. That's what Bayes Theorem does - it helps you make an educated guess, even if you don't know all the facts.\n\nThe normalizing denominator in Bayes Theorem is important because it helps you compare the probability of different outcomes. Without it, you can't properly calculate the likelihood of something being true. The normalizing denominator ensures that all the probabilities add up to one, so that you can make an informed decision about how likely something is to be true.\n\nBayes Theorem is a way to figure out the probability of something being true, even if you don't know all the facts. It uses a normalizing denominator to make sure all the probabilities add up to one, so you can make an educated guess.\n\n* Bayes Theorem is a way to figure out how likely something is to be true.\n* It helps you make an educated guess about something that might be true, even if you don't know for sure.\n* The normalizing denominator in Bayes Theorem is important for properly calculating the likelihood of something being true.\n* Bayes Theorem uses a normalizing denominator to make sure all the probabilities add up to one, so you can make an informed decision.\n\nBayesian Statistics is a branch of statistics that uses probability theory to make inferences about unknown parameters. It is based on the assumption that all data is uncertain, and that \"prior knowledge\" can be used to update the probability of a given outcome.\n\nIn Bayesian Statistics, the probability of an event happening is determined by the prior probability of the event, and the likelihood of the event happening given the data available. This is known as Bayes' Theorem.\n\nThe main advantage of Bayesian Statistics is that it allows for uncertainty in data. It also allows for dynamic updating of probabilities as more information is gathered, and for the inclusion of prior knowledge about a given situation. This makes it well-suited for real-world applications.\n\nBayesian Statistics can be used to make inferences about unknown parameters, such as population proportions, correlations, and effects of interventions. It can also be used to estimate the probability of a certain outcome, and to compare different models.\n\nBayes Theorem is useful when receiving new information because it can be used to update the probability of a given outcome. It allows us to take into account the prior probability of an event, as well as the likelihood of the event happening given the data available. This makes it well-suited for real-world applications, where we need to make decisions with uncertain data. For example, Bayesian Statistics can be used to make inferences about unknown parameters, such as population proportions, correlations, and effects of interventions. It can also be used to estimate the probability of a certain outcome, and to compare different models.\n\nIn Bayes Theorem, the new information and updating beliefs show up in the numerator of the equation, which is made up of the probability of the event given the new information (P(B|A)) multiplied by the probability of the event occurring (P(A)). This allows us to take into account the prior probability of an event, as well as the likelihood of the event happening given the data available.\n\nThe prior probability of an event is shown in the numerator of the Bayes Theorem equation, which is made up of the probability of the event given the new information (P(B|A)) multiplied by the probability of the event occurring (P(A)).\n\nThe formula for Bayes Theorem is:\n\n$$ P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)} $$\n\nBayes Theorem is a way to figure out how likely something is to be true, even if you don't know all the facts. It uses something called the prior probability, which is like how likely you think something is to be true before you get more information. It's like if you were trying to guess what color jelly beans were in a bowl, and you thought there were mostly red ones - that's the prior probability. It helps you make an educated guess, even if you don't know all the facts.\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/Bayesian-Statistics":{"title":"Bayesian Statistics","content":"\nIn [Bayesian Statistics](Bayesian%20Statistics.md) the interpretation of what [Probability](Probability.md) means is that is a statement of *how certain you are that some statement, or proposition is true.* \n\nIn [Bayesian Statistics](Bayesian%20Statistics.md), the [Probability](Probability.md) is in the mind, not in the world.\n\nIt is about changing your mind when [New Information](New%20Information.md) becomes available.\n\nWhen we get [New Information](New%20Information.md) then we should update our [Probability](Probability.md) to take the new information into account.\n\n[Bayesian Statistics](Bayesian%20Statistics.md) tells us how to do this *updating.*\n\n## Probabilities\n\nThe probability we started with is called the [Prior Probability](Prior%20Probability.md), and when we get updated with new information we get a [Posterior Probability](Posterior%20Probability.md) and we can use a [Bayes Box](Bayes%20Box.md) to help us calculate the [Posterior Probability](Posterior%20Probability.md) easily.\n\n#### Imagine the following experiment\n\nWe only know that one of the following * hypotheses* is true\n\n \u003e \n \u003e BB: Both balls are black\n \u003e BW: One ball is black and the other is white\n\nResult:\n\n \u003e \n \u003e D: The ball that was removed from the bag was black\n\nNow let's do an Bayesian analysis of this result:\n\nWe can use the following [Prior Probability](Prior%20Probability.md)\n$$ P(BB) = 0.5 $$ $$ P(BW) = 0.5 $$\n$$ \\frac{8}{3} $$\n\n|Yes|Hello|\n|---|-----|\n|Vector|Matrix|\n|0||\n|1|1|\n|9||\n|2||\n|2|3|\n|2||\n|4||\n|4||\n|6||\n|7||\n|8||\n|||\n\n[Neyman Style Confidence Intervals](Neyman%20Style%20Confidence%20Intervals.md)\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/Bernoulli-Distribution":{"title":"Bernoulli Distribution","content":"\n![Screenshot 2023-03-01 at 6.09.08 PM.png](Image%20Bank/Screenshot%202023-03-01%20at%206.09.08%20PM.png)\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/Bernstein-von-Mises-Theorem":{"title":"Bernstein-von Mises Theorem","content":"\nThe Bernstein-von Mises theorem states that when the sample size is sufficiently large and the prior distribution is \"vague\" (low-information), the posterior distribution from a [Bayesian Statistics](Bayesian%20Statistics.md) analysis is approximately equal to the [Maximum Likelihood](Maximum%20Likelihood.md) (ML) estimator, which is obtained from [Frequentist Statistics](Frequentist%20Statistics.md) inference.\n\nIn other words, as the sample size gets larger, the Bayesian and frequentist approaches to inference converge and produce similar results. This theorem provides a justification for using either Bayesian or frequentist methods in certain situations, and helps bridge the gap between these two approaches to statistical inference.\n\nThis has practical implications for researchers who may have a preference for one approach over the other, or who may be required to use one approach for certain analyses. For example, Bayesian methods may be preferred when prior information about the parameters of interest is available, while frequentist methods may be preferred when no such information is available. However, the Bernstein-von Mises theorem suggests that in large samples with a low-information prior, the choice of method may not matter as much, and the results will be similar regardless of the approach taken.\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/Best-Linear-Unbiased-Estimator":{"title":"Best Linear Unbiased Estimator","content":"\nThe linear unbiased estimator (i.e. an estimator that is a linear function of sample observations) that produces the smallest variance for the estimated parameter\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/Big-Data":{"title":"Big Data","content":"\nModern datasets have large sample sizes, which require new methods of computation, and are high dimensional, which requires new statistical methods.\n\n* Big 𝑛, which means there is a large number of observations\n  \n  When 𝑛 is too large, the previously simple act of computing OLS estimates requires distributed algorithms that can process data spread across multiple machines.\n\n* Big 𝑑 (many variables / high-dimensional data)\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/Binomial-Distribution":{"title":"Binomial Distribution","content":"\n**ChatGPT Made:**\n\nThe Bernoulli and Binomial distributions are two of the most basic discrete probability distributions.\n\nA Bernoulli trial is a binary experiment that has only two possible outcomes, often referred to as \"success\" and \"failure\". The Bernoulli distribution is a discrete probability distribution that describes the outcome of a single Bernoulli trial. The Bernoulli distribution has only two possible outcomes, each with a probability of success (p) or failure (1-p).\n\nThe Binomial distribution, on the other hand, describes the number of successes in a fixed number of independent and identically distributed Bernoulli trials. The Binomial distribution is a discrete probability distribution with parameters n (the number of trials) and p (the probability of success in each trial). The Binomial distribution gives the probability of getting exactly k successes in n trials, for k = 0, 1, 2, ..., n.\n\nIn summary, the Bernoulli distribution describes a single Bernoulli trial, while the Binomial distribution describes the number of successes in a fixed number of independent and identically distributed Bernoulli trials.\n\n![Screenshot 2023-03-01 at 6.09.52 PM.png](Image%20Bank/Screenshot%202023-03-01%20at%206.09.52%20PM.png)\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/Binomial-Variable":{"title":"Binomial Variable","content":"\n\\[0,1] variable\n\n## Statistical Test\n\n#### Exact Binomial Test\n\nFor example (how many farmers support sustainable agriculture):\n\nLet's compare our data with what we know (or think, or **expect**) is **true**\n\n* Expected =\u003e 50/50 =\u003e Null Hypothesis\n* Observed =\u003e 30/70 =\u003e Alternative Hypothesis\n\nP-value = 0.348\n\nThus we have no difference from the null hypothesis in our data\n\n#### Pearson's Chi-squared Goodness-of-fit Test for Count Data\n\nThe problem with the binomial test is that it only can be done on the binomial variable\n\nThis test performs the same thing **within one categorical variable** as the Exact Binomial Test\n\nBut it can also be used between 2 categorical variables or with a variable that has more than two categories (thus no longer binomial). This makes it useful for [Factor Variables](Factor%20Variables.md)\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/Bootstrap":{"title":"Bootstrap","content":"\nOne of the better non-parametric modelling techniques\n\nWITH REPLACEMENT - each observation might be sampled multiple times \n\n\"It is like cross-validation on steroids\" - yuzaR Data Science\n\nThe data will have the same size as the data\n\nCan also deliver robust estimates of model parameters\n\n* Confidence intervals\n\n## Problems\n\nTends to produce estimates with low variance (higher bias) which is the opposite problem to cross-validation\n\nWhich can result in slightly pessimistic estimates of model performance\n\nThus it would show a lower accuracy than with all the data (usually very small)\n\nMore likely to produce conservative estimates\n\nAvoids false positives more often\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/Boxplot":{"title":"Boxplot","content":"\nHelps quantify outliers\n\nHow many outliers does a distributio have\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/Breusch-Pagan-Test":{"title":"Breusch-Pagan Test","content":"\n\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/Capital-Account-Liberalization-and-the-Neoclassical-Growth-model":{"title":"Capital Account Liberalization and the Neoclassical Growth model","content":"\nProducer problem:\n\n$$\\Large max_kf(k) - rk - \\delta k$$\n\nwe can solve this problem by taking FOC w.r.t capital\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/Causal-Inference":{"title":"Causal Inference","content":"\n[Causal Inference](Causal%20Inference.md) \n\nCausal evidence can help us understand positive relationships snd can detemrine how we want t at act given this causal evidence\n\n[Normative](Normative.md) vs [Positive](Positive.md) \n\n### The motivation:\n\n* The need for *causal evidence* \n\nCausal inference helps shape *effective* policies to meet [Normative](Normative.md) objectives.\n\n## Three problems with correlations:\n\n* [Selection Bias](Selection%20Bias.md)\n* [Reverse Causality](Reverse%20Causality.md)\n* [Omitted Variable Bias](Omitted%20Variable%20Bias.md)\n\n[Ordinary Least Squares](Ordinary%20Least%20Squares.md)\n\n## Methodologies:\n\nRanking:\n\n1. [Randomized Control Trial (RCT)](Randomized%20Control%20Trial%20%28RCT%29.md)\n1. *Randomized Encouragement Design*\n1. \"Perfect compliance\" [Instrumental Variable](Instrumental%20Variable.md)\n1. \"Imperfect compliance\" [Instrumental Variable](Instrumental%20Variable.md) ([2SLS](2SLS.md))\n1. [Regression Discontinuity](Regression%20Discontinuity.md) (\"sharp\" or \"fuzzy\")\n1. [Difference-in-differences](Difference-in-differences.md)\n1. Variations on [Difference-in-differences](Difference-in-differences.md) such as [Synthetic Control](Synthetic%20Control.md), *matching* plus DiD, etc.\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/Central-Limit-Theorem":{"title":"Central Limit Theorem","content":"\n\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/Changes-in-the-Labor-Market":{"title":"Changes in the Labor Market","content":"\n## Productivity\n\nTFP Growth (24 min in Notability recording)\n\n$$\\Large y = aK^{\\alpha}L^{1-\\alpha}$$\n\nLog(A) = TFP\n$$\\Large logY = log(A) + \\alpha logK + (1-\\alpha)logL$$\n\n## Employment Growth\n\nJob growth in low-skilled occupations\n\nSkill is synonymous with wage\nSkill is rewarded by wage (skills that are valued)\n\n## Math, People Skills\n\nHigh math, high social biggest positive relative change in employment\n\nLow math, high social is the second most positive relative change\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/Cheating-Approach":{"title":"Cheating Approach","content":"\nTends to understate our uncertainty because we use the entire data set twice.\n\n* THis is when using a histogram to esitmate the *Sampling Model*\n\nThis is a cheating approach to [Sampling Model Uncertainty](Sampling%20Model%20Uncertainty.md)\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/Cigarette-Tax":{"title":"Cigarette Tax","content":"\nNeed variation to measure impact\n\n* Cigarettes taxed at both federal and state levels in U.S.\n* Total revenue (state+fed) of about $35 billion per year, similar to estate taxation\n* Federal tax increased from $0.39 to $1.01 per pack in 2009\n* Variation among states: from 30 cents per pack in VA to $4.35 in NY in 2021\n* Controversial commodity due to health and paternalism concerns\n\n## Research Design\n\nConsider cross-sectional variation\n\nWhat are some possible problems with comparing cigarette prices or quantities between high and low tax states?\n\nPass-trhough rate:\n\n100% pass-trhough means all incidence is on the consumer\n\nInelastic demand\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/Classical-Probability":{"title":"Classical Probability","content":"\n\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/Classical-Statistics":{"title":"Classical Statistics","content":"\nYou do not need to think about any other spins than the one about to happen\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/Conditional-Independence-Assumption-CIA":{"title":"Conditional Independence Assumption (CIA)","content":"\nThe Conditional Independence Assumption in probability theory states that, given certain conditions or events, the probability of two variables is independent of each other. In simpler terms, it means that the occurrence of one event does not impact the probability of the other event happening.\n\nHowever, in some cases, the conditional independence assumption may become an Unconditional Independence Assumption. This happens when the variables become completely independent of each other, regardless of any conditions or events. In other words, the assumption holds true even when there are no conditions or events that could affect the probability of the variables.\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/Conditional-Mean":{"title":"Conditional Mean","content":"\nthe mean of a random variable or a function of random variables given that you know the value of another random variable\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/Conditional-Probability":{"title":"Conditional Probability","content":"\nConditional probability is a fundamental concept in probability theory and it is the probability of an event occurring, given some prior knowledge. This probability can be expressed mathematically using the formula: $$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$ Where $P(A|B)$ is the probability of event A occurring given that event B has already occurred, $P(B|A)$ is the probability of event B occurring given that event A has already occurred, $P(A)$ is the probability of event A occurring, and $P(B)$ is the probability of event B occurring. This formula allows us to calculate the probability of a certain event occurring, given prior knowledge about that event. Furthermore, this formula can be used to determine how likely it is that a specific event will occur, depending on the information that is available. It is also useful in determining how likely it is that, given a certain set of conditions, a specific event will occur. Additionally, it can be used to calculate the probability of several events occurring at the same time, allowing us to better understand the interrelation of multiple events.\n\n$$P(A \\mid B) = \\frac{P(\\text{A and B})}{P(\\text{B})} \\space \\text{| if P(B) \u003e 0 and undefined}$$\n\nIf B = (B1 and B2, ... etc.)\n\nAnd e.g. B2 is false then B = false\n\nIf B = false then with conditional probability (the thing conditioning on being 0) then we are dividing 0/0.\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/Confidence-Interval":{"title":"Confidence Interval","content":"\nIf you make α larger then your confidence interval widens\n\nAlpha = False Discovery Rate\n\nGood science is repeatable\n\nBias violate I.I.D. assumption upon which the sample is based. This violation results in non-accurate results\n\nRejecting h0\n\nalpha/2\n\n1-alpha/2\n\nProbability that the sample estimate is euqal to the true estimate with a certain level of confidence or probability. Thus 95% confidence interval means there is \n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/Confusion-Matrix":{"title":"Confusion Matrix","content":"\nHere are some of the most common performance measures you can use from the confusion matrix. Accuracy: It gives you the overall accuracy of the model, meaning the fraction of the total samples that were correctly classified by the classifier. To calculate accuracy, use the following formula: **(TP+TN)/(TP+TN+FP+FN)**\n\n![confusionMatrxiUpdated.jpg](Image%20Bank/confusionMatrxiUpdated.jpg)\n","lastmodified":"2023-05-25T06:37:17.23865331Z","tags":[]},"/notes/Consumption":{"title":"Consumption","content":"\n## Macroeconomics\n\nReal terms\n\n* Disposable income (income - taxes) or Y - T\n\nMarginal Propensity to Consume:\n\n* Preferences\n* interest rate\n\nDerivative of consumption to disposable income\n\n$$\\Large \\frac{\\partial{C}}{\\partial{(Y-T)}}$$\nMathematically, if we denote consumption as C and disposable income as Y, the derivative of consumption with respect to disposable income is written as dC/dY. This derivative can be positive, negative, or zero.\n\n* Positive derivative (dC/dY \u003e 0): This indicates that consumption increases as disposable income increases. In other words, as individuals or households have more disposable income, they tend to spend more on consumption goods and services.\n\n* Negative derivative (dC/dY \\\u003c 0): This suggests that consumption decreases as disposable income increases. It implies that as individuals or households have more disposable income, they tend to save more or allocate the additional income towards other purposes rather than increasing consumption.\n\n* Zero derivative (dC/dY = 0): This indicates that there is no relationship or dependency between changes in disposable income and consumption. In other words, changes in disposable income do not affect consumption behavior.\n\nThe derivative of consumption with respect to disposable income, denoted as dC/dY, can be interpreted as the marginal propensity to consume (MPC). The MPC represents the proportion of each additional unit of disposable income that is spent on consumption.\n\n0 \\\u003c C'(Y-T) \\\u003c 1 #todo 55:00\n\n$$\\Large max \\space U = \\sum\\_{t=1}^{T} u (Ct)$$\n![Screenshot 2023-05-11 at 1.02.54 PM.png](Image%20Bank/Screenshot%202023-05-11%20at%201.02.54%20PM.png)\n\nThis graph shows that marginal utility is high at low levels of consumption, which means that a small change in consumption will increase utility by a lot (steeper slope)\n\nMarginal \n\nLifetime utility: discounting utility not consumption #todo 1:10\n\n$$\\Large U = \\sum\\_{t=0}^{T} 1/(1u (Ct)$$\n\nUtility : power function\n\n\\#todo on ipad note\n\n#### Impatience\n\nDeclining consumption = impatient people\n\nI discount the future more than the market\n\nMore patient = lower rho\n\nIf output is constant = always have patient economies lending to impatient economies\n\n\\#todo 1:27:00\n\n## The Interest Rate and Consumption Growth\n\nWhen we allow for a nonzero interest rate, it is also useful to allow for a nonzero discount rate. In this analysis, we assume a CRRA utility function\n\nwhere $\\rho$ is the discount rate and $\\theta$ is the coefficient of relative risk aversion\n\nCRRA utility function\n\nConstant Relative Risk Aversion (CRRA) utility is a type of utility function that describes an individual's preference for consumption of goods and services. It is based on the assumption that individuals are risk-averse, meaning they prefer a certain outcome to an uncertain one, and that this aversion to risk is constant over time. The CRRA utility function takes the form:\n\n$$\\Large U(C) = (C^(1-γ)-1)/(1-γ)$$\n\nwhere U is the utility function, C is consumption, and γ is the coefficient of relative risk aversion. The higher the value of γ, the greater the individual's aversion to risk.\n\n**Explain how relative risk aversion affects consumption and when it get bigger how does consumption change**\n\nRelative risk aversion refers to an individual's preference for less risky options over riskier ones, even if the potential gains of the latter are greater. In terms of consumption, relative risk aversion can affect an individual's spending habits.\n\nWhen an individual has a low level of relative risk aversion, they are more willing to take risks and invest in higher-risk assets or opportunities. This may lead to increased consumption as they are more likely to spend money on things like luxury goods or experiences.\n\nOn the other hand, when an individual has a high level of relative risk aversion, they are more hesitant to take risks and may opt for safer investment options or savings. This may lead to decreased consumption as they are less likely to spend money on non-essential items.\n\nAs relative risk aversion increases, consumption tends to decrease as individuals become more cautious with their spending. This is because they prioritize saving and minimizing financial risks over immediate gratification through consumption.\n\n**Explain how the discount rate affects consumption and when it get bigger how does consumption change with a CRRA utility function**\n\nThe discount rate is the rate at which individuals or firms value present consumption over future consumption. A higher discount rate means that individuals or firms place a higher value on present consumption relative to future consumption. This can have a significant impact on consumption decisions.\n\nWhen the discount rate increases, individuals or firms are more likely to prioritize present consumption over future consumption. This means that they may be more likely to spend their money now rather than saving it for later. As a result, an increase in the discount rate can lead to an increase in current consumption.\n\nHowever, when considering a CRRA (Constant Relative Risk Aversion) utility function, the impact of an increase in the discount rate on consumption becomes more complex. The CRRA utility function assumes that individuals are risk-averse and prefer a smooth and stable level of consumption over time.\n\nIn this case, an increase in the discount rate may actually lead to a decrease in current consumption if it leads to greater uncertainty about future income or if it makes individuals more risk-averse. This is because higher discount rates make future income less valuable, which can lead individuals to save more in order to smooth out their level of consumption over time.\n\nOverall, the impact of changes in the discount rate on consumption depends on a variety of factors, including individual preferences and attitudes towards risk.\n\n## Maximization Problem\n\n#### Lagrangian\n\n#### FOCs:\n\nThis analysis implies that once we allow for the possibility of that real interest rate and the discount rate are not equal, consumption need not to be a random walk.\n\nr = real interest rate\n$\\rho$ = discount rate\n\nConsumption is rising when r \u003e $\\rho$, we say that the individual is patient, spend more tomorrow and less today\n\nConsumption is falling when r \\\u003c $\\rho$, impatient, spend more today than tomorrow\n\n## Not sure where to put this yet:\n\n\\#todo \n\nFor the estimated consumption function (𝐶𝑖 = 𝑎 + 𝑏𝑌𝑖 + 𝑢𝑖), the values of a and b are supposed to be:\n\n(a) 𝑎 \u003e 0, 𝑏 \\\u003c 1\n\nThe intercept term (a) represents the autonomous consumption, which is the consumption level that does not depend on income. It is expected to be positive since people have basic needs that must be satisfied even if they have no income.\n\nThe slope coefficient (b) represents the marginal propensity to consume, which is the fraction of additional income that is spent on consumption. It is expected to be positive but less than one since people tend to save a portion of their income as a precautionary measure or to invest it.\n\nTherefore, the correct option is (a) 𝑎 \u003e 0, 𝑏 \\\u003c 1.\n\nWhat is the permanent income hypothesis in economics\n\nThe permanent income hypothesis is an economic theory that suggests that an individual's consumption patterns are determined by their long-term or permanent income, rather than their current income or temporary fluctuations in income. According to this theory, individuals base their consumption decisions on the average level of their expected future income, rather than just their current earnings.\n\nThis theory was first proposed by economist Milton Friedman in 1957 and has since been widely studied and debated in the field of economics. The permanent income hypothesis suggests that individuals save more when they have a higher permanent income, as they can afford to set aside more money for the future without reducing their current standard of living.\n\nOverall, the permanent income hypothesis provides a framework for understanding how individuals make consumption decisions over time and how these decisions are influenced by factors such as earnings, savings, and expectations about future income levels.\n\nWhy is the derivative of disposable income equal to MPC\n\nThe marginal propensity to consume (MPC) is defined as the proportion of additional disposable income that is spent on consumption. The derivative of disposable income with respect to consumption represents the rate at which consumption changes in response to changes in disposable income. Therefore, the derivative of the consumption function with respect to disposable income is equal to MPC because it measures the change in consumption resulting from a change in disposable income. In other words, as disposable income increases by a certain amount, the derivative of consumption represents how much additional spending will occur due to that increase, and this is precisely equal to the MPC.\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Contigent-Valuation":{"title":"Contigent Valuation","content":"\nSurvey with open-ended question\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Continuous-Uniform-Random-Variable":{"title":"Continuous Uniform Random Variable","content":"\n\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Correlation-Matrix-Heat-Map":{"title":"Correlation Matrix Heat Map","content":"\nlibrary(ggplot2)\ncor_matrix \\\u003c- cor(PriceData\\[,c(\"metalc\", \"fuelc\", \"crudeoilc\", \"naturalgasc\", \"coalc\", \"fuell\", \"crudeoill\", \"naturalgasl\", \"coall\")])\nggplot(data = melt(cor_matrix), aes(x=Var1, y=Var2, fill=value)) + \ngeom_tile() + \n\\#scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0, limit = c(-1,1), space = \"Lab\", name=\"Correlation\") + \ntheme_minimal() + \ntheme(axis.text.x = element_text(angle = 45, vjust = 1, \nsize = 12, hjust = 1)) +\ncoord_fixed()\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Counterfactual-Prediction":{"title":"Counterfactual Prediction","content":"\n\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Cross-Validation":{"title":"Cross-Validation","content":"\nWe split the whole training set into **multiple validation sets** (known as folds)\n\nUse [Validation Set](Validation%20Set.md) to evaluate the model that was created/analyzed on the remaining folds (which can be thought of as the training set)\n\nThus in the picture below: \n\n* Fold 1,2,3 = Analysis Set (can be thought of as a training set)\n* Fold 4 = Assessment Set (can be thought of as a validation set)\n\nWe only want to use the test set once after completing our model (transformations, adding predictors etc.)\n\nThis is why we want to use a cross-validation which is a more extensive way of using a validation set since it uses multiple randomly sampled validation sets, thus allowing us to better gauge our model's performance. (Kind of like a robustness check)\n\nThis avoids having a model perform well on a validation set by random chance, taking the average of the performance (through a metric such as Out-of-sample deviance, RMSE, MSE etc.) means we get a better idea of what our model actually performs like\n\n## Simplified Version:\n\n![Screenshot 2023-05-07 at 4.24.49 PM.png](Image%20Bank/Screenshot%202023-05-07%20at%204.24.49%20PM.png)\n\n## 4-fold Cross-Validation (k=4)\n\nThis produces multiple validations (e.g. 4 in the picture below)\n\n![Screenshot 2023-05-07 at 4.26.50 PM.png](Image%20Bank/Screenshot%202023-05-07%20at%204.26.50%20PM.png)\n\nWe can take the average of the validations using a metric (e.g. Out-of-sample Deviance)\n\nThis means we can get a better idea of our model's performance\n\nAllows us to find out how well \n\n## Coding\n\n````r\n# Creating the splits for cross-validation\nset.seed(0)\nfolds \u003c- vfold_cv(df_train, v = 10) # 10-fold cross-validation\n\nfolds$splits[[1]] # visualize the split\n\n# OR - get repeated cross-validation (cv tends to produce noisy or high variance estimates)\nset.seed(0)\nfolds \u003c- vfold_cv(df_train, v = 10, strata = origin, repeats = 10) # 10-fold cross-validation\n\nfolds$splits[[1]] # visualize the split\n\n\n\n# set up a model\nlm_fit \u003c-\n\tlinear_reg() %\u003e%\n\tset_engine('lm') %\u003e%\n\tset_mode('regression')\n\n# create a worflow\nmpg_wf \u003c-\n\tworkflow() %\u003e%\n\tadd_model(lm_fit) %\u003e%\n\tadd_formula(mpg ~ origin * horsepower)\n\n# fit model to folds\ntrained_models \u003c- fit_resamples(object = mpg_wf,\n\t\t\t\t\t\t\t   resamples = folds)\n\n# get performance metrics\ntrained_models %\u003e%\n\tcollect_metrics(summarize = FALSE)\n\n````\n\n## Problems with cross-validation\n\nIt tends to produce noisy or high variance estimates\n\nWhich is why we can run repeated cross-validation, meaning we run e.g. 10-fold validation multiple times\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Cumulative-Density-Function-CDF":{"title":"Cumulative Density Function (CDF)","content":"\nA function that describes the probability that a random variable is less than or equal to a certain value. This can be thought of as a cumulative probability distribution where the probability of a given event is the area under a curve up to the point of the event. For example, if the CDF of a variable is 0.2 at a point x, then the probability of the variable being less than or equal to x is 0.2.\n\nThink of it this way: a random variable is like a box with lots of numbers in it. Every time you reach in and pick a number, you don't know what you're going to get - it could be a big number or a small number, but it's always random. The cumulative density function (CDF) is a way to measure the probability of a random variable being less than or equal to a certain value. It's a cumulative probability distribution that tells you the probability of a given event by looking at the area under a curve up to the point of the event. For example, if the CDF of a variable is 0.2 at a point x, then the probability of the variable being less than or equal to x is 0.2.\n\nCDF cannot go down\n\nwould be like abosrbindg a negative probability\n\n* CDF of discrete random variables look like step function\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Current-Account":{"title":"Current Account","content":"\n\"Magic of the current account\" - Galina Hale\n\nCA = EX - IM (ignoring net factor payments)\n\nUS has negative CA = net importer\n\nCA \u003e 0 = net exporter\n\n\\| Current Account  | Type of Country | Borrow vs. Lend             | Consumption |\n\\| ---------------- | --------------- | --------------------------- |\n\\| CA \u003e 0           | Net Exporter    | Net Borrower (overspending) |\n\\| CA \\\u003c 0           | Net Importer    | Net Lender (underspending)  |\n\\| **CA = EX - IM** | **CA = S - I**  | **CA = Income - Spending**  |\n\n$$\\Large Y = C + I + G + NX $$\n$$\\Large Y = C + I + G + CA - NFP $$\ny - c - g = i + ca - nfp\n\nY = income, output\n\nC + I + G = spending\n\nCA = income - spending\n\n$$ \\Large S = Y - C - G $$\n$$ \\Large S = I + CA $$\n$$ \\Large CA = S - I $$\n\nCurrent accoutn decfiit = negative net exports\n\nLess savings than investment\n\nThe current account (CA) in macroeconomics refers to the balance of trade between a country and its trading partners, including the net exports of goods and services, net income from abroad, and net transfer payments. The equation CA = S - I represents the relationship between the current account balance and national savings (S) and investment (I).\n\nIn this equation, national savings refer to all savings made by households, businesses, and the government within a country. Investment refers to all spending on capital goods such as machinery, buildings, and infrastructure.\n\nWhen a country has a positive current account balance (i.e., it exports more than it imports), it means that it is earning more income from abroad than it is spending on imports. This surplus can be used to fund investments within the country or saved for future use. Therefore, in this situation, national savings (S) are greater than investment (I), resulting in a positive current account balance.\n\nConversely, when a country has a negative current account balance (i.e., it imports more than it exports), it means that it is spending more on imports than it is earning from exports. To finance this deficit, the country may need to borrow from other countries or use its own reserves. In this situation, investment (I) is greater than national savings (S), leading to a negative current account balance.\n\nTherefore, in macroeconomics, CA = S - I represents the relationship between a country's current account balance and its levels of national savings and investment.\n\n\\#todo 36:00\n\n#### Intertemporal Approach to Current Account\n\nIf output is assumed constant\n\n[Permanent Income Hypothesis](Permanent%20Income%20Hypothesis.md) - how is this related #todo \n\nCA \\\u003c 0\n\n$$\\Large C\\_{today} \u003e C\\_{tomorrow}$$\n\nCA \u003e 0\n\n$$\\Large C\\_{today} \\\u003c C\\_{tomorrow}$$\n\nCA = 0 \n\nS = I -\u003e r\n\nGlobal Capital Markets sets the interest rate\n\nworld interest rate\n\n\\#todo 43:00\n\nReal interest rate = in a global equilibrium equal \n\nlong-run equilibrium \n\nDetermined by productivity\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Data-Manipulation-in-R":{"title":"Data Manipulation in R","content":"\n## Subsetting the data\n\n#### Based on column value\n\n*Base R:* \n\n````R\n# The `%in%` operator is a binary operator in R that returns a logical vector indicating whether each element of the first vector is found in the second vector or not. In this case, it is used to check whether each value of the `Rating` column of `countries_df` is present in the vector `c(\"BBB+\", \"BBB-\")`.\n\nbbbrating_countries \u003c- countries_df$Country[countries_df$Rating %in% c(\"BBB+\", \"BBB-\")] \n\n# Using the grepl regex method\nbbbrating_countries \u003c- bond_ratings_df$Country[grepl(\"BBB[+-]\", bond_ratings_df$Rating)]\n````\n\nThe second method here uses [Regular Expression](Regular%20Expression.md)\n\n#### In a vector\n\n*Base R:*\n\n````R\n# vector of column names to select \ncol_names \u003c- c(\"col1\", \"col3\", \"col5\") # select columns in the data frame that match the names in the vector \ndf_subset \u003c- df[, names(df) %in% col_names]\n````\n\n*Tidyverse*:\n\n````R\n\n# vector of column names to select \ncol_names \u003c- c(\"col1\", \"col3\", \"col5\") # select columns in the data frame that match the names in the vector \ndf_subset \u003c- df %\u003e% select(names(.) %in% col_names)\n````\n\nFind most common value in a column based on value from another column\n\n````R\nnames(which.max(table(weo_clean$Subject.Descriptor[weo_clean$WEO.Subject.Code == \"GGXCNL_NGDP\"])))\n````\n\n````R\nas.character(weo_clean$Subject.Descriptor[weo_clean$WEO.Subject.Code == \"GGXCNL_NGDP\"][1])\n````\n\n## Drop NA\n\n````R\nlibrary(tidyverse)\ndf %\u003e% drop_na(if_all(everything(), is.na))\n\n````\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Data-Science":{"title":"Data Science","content":"\n## Working with Data\n\n[Tidy Data](Tidy%20Data.md)\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Data-Splitting":{"title":"Data Splitting","content":"\nIn R:\n\ny_train \\\u003c- train$Purchase\nx_train \\\u003c- train\\[, -which(names(train) == \"Purchase\")]\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Derived-Demand":{"title":"Derived Demand","content":"\nDemand for a factor of produciton that occurs becuase of the mdeand for noathe rgood\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Difference-in-differences":{"title":"Difference-in-differences","content":"\nEasy to use\nLess credible than many other methods\n\nOften used when nothing was random\n\nNearly always with panel data\nHigher-ranked methods can be used just in cross-sectional data \n\nThe identifying assumption of DiD is that the trends in the\noutcome variable – rather than its level – would have\nremained identical absent treatment\n\n## Parallel trends\n\nHow would treatment group have evolved in absence of the treatment\n\nDifficult/impossible assumption to verify - state of the world we cannot observe\n\n[Fixed Effects](Fixed%20Effects.md)\n\n$$\\Large y\\_{it} = \\beta_0 + \\beta_1 Post_t + \\beta_2 Treat_t + \\beta_3 (Post \\times Treat)\\_{it}$$\n\n||Pre|Post|\n|--|---|----|\n|Treat|$\\beta_0 + \\beta_2$|$\\beta_0 + \\beta_1 + \\beta_2 + \\beta_3$|\n|Control|$\\beta_0$|$\\beta_0 + \\beta_1$|\n||b2|b2 + b3|\n|Difference|**b3**||\n\n[Triple Difference-in-Difference](Triple%20Difference-in-Difference.md)\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Discrete-Choice-Models":{"title":"Discrete Choice Models","content":"\nYou can have univariate discrete choice models:\n\nWhich I believe are e.g. Logit and Probit models\n\nAnother version are [Multivariate Discrete Choice Models](Multivariate%20Discrete%20Choice%20Models.md) \n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Discrete-Random-Variable":{"title":"Discrete Random Variable","content":"\nmean = value * probability, summed\n\nHeights of the jumps\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Distribution-of-Residuals":{"title":"Distribution of Residuals","content":"\nAuto correlation\n\nHetero/Homoskedacity = which could affect the standard errors and thus confidence intervals\n\nIf not iid and normally distributed errors then could indicate something like omitted variables\n\nReturns take out serial returns\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Distributions":{"title":"Distributions","content":"\n## What is the difference between a Probability Mass Function and a Probability Density Function\n\nA Probability Mass Function (PMF) is used to describe the probability distribution of a discrete random variable. It gives the probability of each possible outcome of an experiment or event. For example, the PMF of rolling a fair six-sided die would be:\n\nP(X=1)=1/6\nP(X=2)=1/6\nP(X=3)=1/6\nP(X=4)=1/6\nP(X=5)=1/6\nP(X=6)=1/6\n\nA Probability Density Function (PDF), on the other hand, is used to describe the probability distribution of a continuous random variable. It gives the probability density at each possible value of the random variable. For example, the PDF of a normal distribution with mean μ and variance σ^2 would be:\n\nf(x) = (1/(σ√(2π))) * e^(-(x-μ)^2/(2σ^2))\n\n**The key difference between PMF and PDF is that PMF gives actual probabilities for each possible outcome, while PDF gives probabilities for ranges of values.**\n\nIn summary, PMF is used for discrete random variables, while PDF is used for continuous random variables.\n\n## What is the difference between a PDF and CDF and what do they both show you\n\nA PDF (Probability Density Function) is a function that describes the probability of a random variable taking on a certain value. It tells us the likelihood of a particular event occurring, given the distribution of the data. In other words, it shows the relative frequency of occurrence of different values in a dataset.\n\nOn the other hand, a CDF (Cumulative Distribution Function) gives us the cumulative probability of a random variable being less than or equal to a certain value. This means that it shows the probability that an event will occur up to and including a particular value.\n\nIn simpler terms, imagine you are flipping a coin. The PDF would tell you how likely it is to get heads or tails on each individual flip. The CDF would tell you how likely it is to get heads or tails after multiple flips - for example, after 10 flips, there is a 50% chance of getting 5 or more heads.\n\nOverall, both PDF and CDF are important tools in statistics as they provide information about the distribution and probability of events occurring in datasets.\n\nPDF or Probability Density Function is a function that describes the likelihood of a random variable taking on a particular value. It is used to describe the probability distribution of continuous random variables. The area under the curve of a PDF represents the probability that a random variable will fall within a specific range of values.\n\nCDF or Cumulative Distribution Function, on the other hand, is the probability distribution function that describes the cumulative probability of a random variable taking on a value less than or equal to a given value. It gives us the probability that our random variable takes on any value less than or equal to our given value. The CDF is always increasing from 0 to 1 and its graph shows how fast our PDF accumulates probabilities as we move from left to right.\n\nIntuitively, PDF tells us how likely it is for our random variable to take on any particular value, while CDF tells us how likely it is for our random variable to take on any value less than or equal to our given value.\n\n## Probability Density Function (PDF)\n\n[Probability Density Function](Probability%20Density%20Function.md) can be used to show\n\n* Used for continuous variables\n* Y-axis on PDF shows gradient of CDF\n* Derivative of the CDF is the PDF\n\n## Cumulative Density Function (CDF)\n\n[Cumulative Density Function (CDF)](Cumulative%20Density%20Function%20%28CDF%29.md) \n\nShows how much of the distribution has been accumulated\n\n* Higher gradient (steeper slope) = higher density\n* Flatness in CDF indicates no mass on PMF\n* Integrate the PDF to get the CDF\n\n---\n\nThe graph can be described using words such as:\n\n* Symmetric\n* Skewed\n* Heavy-tailed\n* Unimodal\n* \n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/EITC":{"title":"EITC","content":"\nEITC stands for Earned Income Tax Credit. It is a refundable tax credit for low to moderate-income working individuals and families. The EITC is designed to provide financial assistance to those who are struggling to make ends meet, while also incentivizing work.\n\nAn example of the EITC in terms of economics would be a family with two children and an income of $25,000 per year. Without the EITC, this family might struggle to pay for basic necessities like food, housing, and healthcare. However, with the EITC, they may qualify for a credit of up to $5,600, which could significantly improve their financial situation and help them meet their basic needs.\n\nOverall, the EITC is an important tool in reducing poverty and promoting economic mobility by providing targeted financial assistance to those who need it most.\n\n## Model\n\n#### Basic Setup\n\n* Utility: C\n\n---\n\nSure, let's break down this model step by step. This is a labor supply model with an Earned Income Tax Credit (EITC) type program. The aim of the model is to find out how different individuals (\"types\") choose to supply labor (i.e., how many hours they work) given their personal preferences for leisure and the structure of the EITC program.\n\nLet's start with some definitions:\n\n1. **Utility function**: The utility function is given by C + αln(HL), where:\n   \n   * C is a composite consumption good, which is typically bought with income from working.\n   * H represents available hours, which could be used for work or leisure.\n   * L is hours worked.\n   * α is a parameter that represents the individual's preference for leisure. The larger α is, the more the individual values leisure relative to consumption.\n   * The term ln(HL) represents the logarithm of leisure hours. Since H is the total hours available and L is hours worked, H - L would be hours of leisure. Therefore, this term increases as leisure increases.\n1. **EITC Program**: This is a simplified model of a real-world policy where the government provides a subsidy to low-income workers. In this model, the EITC program works as follows:\n   \n   * For income up to $1000, there is a 40% subsidy. This means that for every dollar earned, the worker gets an additional 40 cents from the government. This is known as the phase-in region.\n   * For income between $1000 and $1200, the benefit is fixed at $400.\n   * For income above $1200, the benefit starts to decrease at a rate of 20% for each dollar earned, until it becomes zero at income=$3200. This is known as the phase-out region.\n\nThe model is solved by finding the optimal labor supply in different segments of the EITC program:\n\n1. **Non-participation**: Some individuals might choose not to work at all. According to the model, this happens if the marginal rate of substitution (MRS) is greater than or equal to 1.4w, where w is the wage rate. The MRS represents the amount of consumption the individual is willing to give up for an additional hour of leisure. If the MRS is high, it means the individual highly values leisure and might choose not to work.\n\n1. **Phase-in region**: Other individuals might choose to work up to the point where their income is $1000 (the end of the phase-in region). These are individuals who value leisure less than the non-participants, but still relatively high. The MRS at the end of the phase-in region is equal to 1.4w, and individuals with an α that corresponds to this MRS will choose to work up to this point.\n\nFinally, the labor supply in each region is found by equating the MRS to the effective wage (which is the wage plus the EITC benefit). The effective wage in the phase-in region is 1.4w, and so the labor supply is found by rearranging the MRS equation to find L.\n\nThe key takeaway from this model is that people's labor supply decisions depend on their personal preferences for leisure (captured by α) and the structure of the EITC program.\n\nThe model is further analyzing the labor supply decisions of individuals based on their preferences for leisure (α), specifically in the first kink point and the flat segment of the EITC program.\n\nLet's break down these two segments:\n\n1. **First kink point (C)**: This is the point where the phase-in region ends and the flat benefit region begins, i.e., where the income is $1000. At this point, the slope of the indifference curve (which represents the tradeoff the individual is willing to make between consumption and leisure) is between w (the wage rate) and 1.4w (the effective wage rate in the phase-in region).\n   \n   The upper bound α that makes the marginal rate of substitution (MRS) equal to 1.4w at this kink was found in the previous segment (B). To find the lower bound α that makes the MRS equal to w, the model sets the MRS equal to w when the labor supply L is 1000/w (which is the amount of labor supplied at the first kink). Solving this equation for α gives α = wH - 1000. So, individuals with an α in the range \\[wH - 1000, 1.4wH - 1400] will choose to work until their income is $1000.\n\n1. **Flat segment (D)**: This is the region where the EITC benefit is fixed at $400, i.e., where the income is between $1000 and $1200. The upper bound α for this segment was found in the previous segment (C) as α = wH - 1000.\n   \n   To find the lower bound α, the model sets the MRS equal to w when the labor supply L is 1200/w (which is the amount of labor supplied at the second kink). Doing so, you find that individuals with an α in the range \\[wH - 1200, wH - 1000] will choose to work in this region. The labor supply for these individuals is given by L = H - α/w, which is found by equating the MRS to the wage rate w.\n\nThese sections of the model continue to illustrate how individuals' labor supply decisions are influenced by their personal preferences for leisure and the structure of the EITC program. In particular, it shows that the kinks in the EITC schedule, where the effective wage rate changes, can lead to jumps in the labor supply.\n\nThe Marginal Rate of Substitution (MRS) is the rate at which an individual is willing to give up one good (in this case, leisure time) in exchange for another good (in this case, consumption), while maintaining the same level of utility or satisfaction. \n\nIn this model, if the MRS is greater than or equal to 1.4w, it means that an individual values their leisure time so highly that they would need to be compensated at a rate of at least 1.4w for every hour of leisure they give up to work. \n\nRemember that 1.4w is the effective wage during the phase-in region of the EITC program, where the government is supplementing 40% of the individual's wage. \n\nSo, if an individual's MRS is greater than or equal to 1.4w, it means that they value their leisure time at least as much as the enhanced wage they would receive if they chose to work. In other words, the additional consumption they could get from working and earning income (even with the EITC supplement) isn't enough to compensate for the loss of leisure time. As a result, these individuals choose not to participate in the labor market at all—they supply zero hours of labor (L=0).\n\nThis demonstrates one of the fundamental trade-offs in economics: individuals must decide how to allocate their scarce resources (in this case, time) between different options (in this case, work and leisure) based on their personal preferences and the available opportunities.\n\nThis part of the model discusses two segments: \n\n1. **Second kink (E)**: This is the point where the flat segment of the EITC program ends, and the phase-out region begins, i.e., where the income is $1200. The model notes that there is no individual type that will choose a labor supply in the phase-out region. The reason is that the marginal benefit of working additional hours (which is the wage minus the reduction in the EITC benefit) is lower than the marginal cost (which is the loss of leisure time). This makes it not worthwhile for individuals to work additional hours in the phase-out region.\n\n1. **Ineligible (G)**: This is the segment where individuals earn too much to qualify for the EITC program. The model wants to find the individual type that is indifferent between working in segment G and at the second kink point E. To do this, it equates the utility of working in segment E and G.\n   \n   The utility at segment E (UE) is straightforward to calculate because the consumption and labor supply are known (C = $1600 and L = 1200/w). The utility at segment G (UG) is calculated by plugging the expressions for consumption and labor supply into the utility function. However, equating UE and UG does not yield a closed-form solution for α.\n   \n   Therefore, the model suggests using a numerical method (a grid search) to find the value of α that makes an individual indifferent between working in segment G and at the second kink point E. In the given example (where H = 2000 and w = 10), the cutoff α is 15058, and the corresponding labor supply in segment G is 494 hours.\n\nThe model concludes by noting that the EITC program can cause individuals to reduce their labor supply significantly to maximize their EITC benefit and enjoy more leisure time. In the given example, a worker who would have worked 493 hours without the EITC program would choose to work only 120 hours to maximize the EITC benefit. This demonstrates one of the potential unintended effects of such programs: they might discourage work among some individuals.\n\nI apologize for any confusion; that statement was a bit unclear. Let's correct it:\n\nIn the given example, when H = 2000 and w = 10, the cutoff α for segments E (the second kink point) and G (ineligible for EITC) is found to be 15058. This means that a worker with this value of α is indifferent between working at the second kink point (where they would work 1200/w = 120 hours to earn $1200 and maximize their EITC benefit) and working in segment G.\n\nIf this worker was in segment G (ineligible for EITC), their labor supply would be L = H - α/w = 2000 - 15058/10 = 494 hours. So, this worker, who without the EITC program would choose to work 494 hours, will now choose to work only 120 hours to maximize the EITC benefit and enjoy more leisure time.\n\nThe main point is that the EITC program can create disincentives for work at certain income levels. Although it provides additional income for low-income workers, the phase-out of the EITC benefit can lead some individuals to reduce their labor supply to stay within the range that qualifies for the maximum benefit.\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Economic-Growth":{"title":"Economic Growth","content":"\n* Savings rate\n* Population growth\n* Innovation (A)\n* Knowledge accumulation (intangible capital stock)\n* Human capital accumulation\n* Natural resources (resource curse, Dutch disease)\n  * Industrial policies\n* Land\n* Specialization\n* Property rights \u0026 other institutions\n* Financial system development: S -\u003e I.\n* Small business policies\n* Capital controls\n  * Financial OPenness\n* Infrastructure investment\n* Geography\n* Capital Allocation Efficiency\n* Social Infrastructure\n  * markets are not pricing anything so no indication of MPK\n  * G tends to be low\n  * \\#todo 1:00:00\n* Distance to equator\n\nsources of economic growth1. Innovation and Technological Advancement: Innovation and technological advancement play a significant role in the growth of an economy. It helps businesses to increase productivity and efficiency, which leads to increased output and profits.\n\n2. Investment in Physical Capital: Investment in infrastructure, such as transportation, communication networks, and energy supply systems, can increase productivity by reducing transport costs, improving connectivity, and ensuring a reliable power supply.\n\n2. Human Capital Development: Human capital is the knowledge, skills, and abilities that individuals possess. Investing in education and training programs can lead to a more skilled workforce that can be more productive and innovative.\n\n2. Natural Resource Development: Natural resources such as oil, gas, minerals, and timber can contribute significantly to economic growth if they are efficiently managed.\n\n2. Efficient Government Policies: Government policies such as tax incentives for businesses or investments in critical sectors of the economy can help promote economic growth.\n\n2. International Trade: International trade provides access to larger markets for goods and services thereby increasing demand which leads to an increase in production output.\n\n2. Political Stability: A stable political environment is essential for economic growth since it attracts foreign investors who are willing to invest their money without fear of instability or social unrest.\n\n2. Entrepreneurship \u0026 Small Business Development: Small businesses play a crucial role in promoting economic growth by creating jobs opportunities and boosting the local economy. \n\n2. Research \u0026 Development (R\u0026D): Investing in R\u0026D helps companies develop new technologies or improve existing ones that can lead to increased productivity while boosting economic growth over time.\n\n2. Access to Finance \u0026 Credit: Access to finance enables businesses to invest in infrastructure development while credit facilities help individuals start their own businesses or invest in existing ones leading to increased employment opportunities which ultimately leads to economic growth of a country or region.\n\n[Greenfield FDI](Greenfield%20FDI.md)\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Education":{"title":"Education","content":"\n## Gap between college and high school growing\n\nLog wage gap is growing\n\n## Educational Attainment stagnating\n\nYears of Schooling by Birth Cohort\n\nCollege attendance stagnating\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Eigenvector":{"title":"Eigenvector","content":"\nAny vector that is only scaled (not transformed?) by a matrix is called an eigenvector of that matrix\n\nThe scale factor is the eigenvalue \n\n![Screenshot 2023-03-04 at 12.41.14 PM.png](Image%20Bank/Screenshot%202023-03-04%20at%2012.41.14%20PM.png)\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Electricity-Strategy-Game":{"title":"Electricity Strategy Game","content":"\n\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Elemental-Outcomes":{"title":"Elemental Outcomes","content":"\n[Equipossible](Equipossible.md) outcomes and elemental outcomes are not the same.\n\nElemental outcomes are the possible outcomes of a random experiment or trial, which are mutually exclusive and collectively exhaustive. In other words, every possible outcome of the experiment can be described by a single, unique elemental outcome.\n\nEquipossible outcomes, on the other hand, are outcomes that have an equal probability of occurring. This means that if there are n equipossible outcomes, each of them has a probability of 1/n of occurring.\n\nWhile it is often assumed that the elemental outcomes of a random experiment are equipossible, this is not always the case. In some situations, the elemental outcomes may have different probabilities of occurring. For example, in a biased coin toss, the elemental outcomes of \"heads\" and \"tails\" are not equipossible, as the coin is more likely to land on one side than the other.\n\nIn summary, elemental outcomes are the possible outcomes of a random experiment, while equipossible outcomes are outcomes that have an equal probability of occurring. While these concepts are related, they are not the same thing.\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Empirical-Research-Design":{"title":"Empirical Research Design","content":"\nFormulate clear research design\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Entropy":{"title":"Entropy","content":"\nEntropy is a measure of uncertainty in a probability distribution. It measures the amount of randomness in a system and is closely related to the concept of information. The entropy of a probability distribution is the expected value of the information contained in a random variable. In other words, it's a measure of how much information can be gained from the distribution. It can be thought of as a measure of how unpredictable a system is. For example, a coin toss has an entropy of 1 bit because there is only two possible outcomes (heads or tails). On the other hand, a dice roll has an entropy of 2.58 bits because there are six possible outcomes. Entropy can also be used to measure the amount of uncertainty in a system by looking at how evenly distributed the probabilities are. The more evenly distributed the probabilities, the higher the entropy.\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Equally-Likely-Model":{"title":"Equally-Likely Model","content":"\nAlso known as ELM\nEqual to the [Pascal-Fermat Probability](Pascal-Fermat%20Probability.md) \n\nIngredients:\n\n1. An experiment not yet performed with (to you, given [Background Information](Background%20Information.md) $\\mathcal{B}$) an [uncertain](Uncertainty.md) outcome.\n1. You can list/enumerate all possible outcomes of the experiment $\\mathcal{E}$ given $\\mathcal{B}$ in such a way that you regard all of them as what [Equipossible](Equipossible.md), thus they are [Elemental Outcomes](Elemental%20Outcomes.md).\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Equipossible":{"title":"Equipossible","content":"\nYou have no reason to that supports to believe that any outcome would be favored over another.\n\nThese can be called [Elemental Outcomes](Elemental%20Outcomes.md).\n\nThere has to be an experiment that people would call a [Random Experiment](Random%20Experiment.md).\n\nThen for a *True/False proposition* the [Pascal-Fermat Probability](Pascal-Fermat%20Probability.md) that A is true given your [Background Information](Background%20Information.md) which is also called the [Classical Probability](Classical%20Probability.md) is simply obtained by counting.\n$$P\\_{pf}(A \\mid B) = P\\_{c}(A \\mid B)$$\n\nThis is how you 'count' to get the [Classical Probability](Classical%20Probability.md):\n$$\\frac{\\text{\\# of outcomes favored to A}}{\\text{Total \\# of Elemental Outcomes}}$$\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Exchange-Rates":{"title":"Exchange Rates","content":"\nExchange rate is a zero-sum game\nIf one person currencies appreicates by definitiont he otehr depreciates\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Expected-Value":{"title":"Expected Value","content":"\nExpectation in statistics?\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Exponential-Distribution":{"title":"Exponential Distribution","content":"\n[![Solved: Probability - Exponential Distribution Derive The ... | Chegg.com](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fd2vlcm61l7u1fs.cloudfront.net%2Fmedia%2F34e%2F34e3ced3-6e4b-4207-8d67-16f43e0471a5%2Fphpnbnszx.png\u0026f=1\u0026nofb=1\u0026ipt=31d1ab084d59c3001e635d9c49a9a9cf3e19a094f8042bccf0053ef1ae2f021a\u0026ipo=images)](https://d2vlcm61l7u1fs.cloudfront.net/media/34e/34e3ced3-6e4b-4207-8d67-16f43e0471a5/phpnbnszx.png)\n\n[Probability Density Function](Probability%20Density%20Function.md):\n\n[![Exponential Distribution - 1.55.0](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fwww.boost.org%2Fdoc%2Flibs%2F1_55_0%2Flibs%2Fmath%2Fdoc%2Fgraphs%2Fexponential_pdf.png\u0026f=1\u0026nofb=1\u0026ipt=5fc4692ef70959af1d54a2ee5215ca8eee343f71ef0616ec575535e2b94d5dcd\u0026ipo=images)](https://www.boost.org/doc/libs/1_55_0/libs/math/doc/graphs/exponential_pdf.png)\n\nPDF is highest at small y_i more proabble than large y_i\n\ny_i is the random variable in this case\n\n$\\lambda$ now replaces $\\theta$  1 x K scalar\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Exports-slowing-down":{"title":"Exports slowing down","content":"\n#### Rising Interest Rates\n\nExports will slow = less investment, costlier to borrow\n\n#### Financial instability\n\nTrade Credit from buyer or bank\n\nUncertainity, less investment, less export growth\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Factor-Variables":{"title":"Factor Variables","content":"\nI believe this is a way of saying categorical in R #todo\n\nText data cannot be discrete. That wording is reserved for numerical data.\n\nAlso known as a categorical variable\n\n2 categories are sometimes called a [Binomial Variable](Binomial%20Variable.md) (which I believe means it is drawn from the [Binomial Distribution](Binomial%20Distribution.md) #todo)\n\nAll you can do is **count** them\n\nWith the counts we can get proportions or probabilities\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Fairness-Assumption":{"title":"Fairness Assumption","content":"\n1. Fairness / Independence of spins\n\n1. [Equipossible](Equipossible.md) of outcomes\n\n[Pascal-Fermat Probability](Pascal-Fermat%20Probability.md) just needs equipossibility of outcomes\n\n[Frequentist Probability](Frequentist%20Probability.md) needs **both** assumption of independence of spins and [Equipossible](Equipossible.md) outcomes.\n\nWithout fairness all bets are off.\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Fixed-Effects":{"title":"Fixed Effects","content":"\nEquivalent to dummy variable per year and per state\n\nIdentification obtained from witin-state variation over time\nCalled two-way fixed effects estiamte (panel and time FE)\n\nNote: common changes that apply to all groups (e.g. fed tax change) captured by time dummy; not a source of variation that identifies $\\large \\beta$.\n\n[Goodman-Bacon critique](Goodman-Bacon%20critique.md)\n\nChange in some states and not others\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Food-Stamps-and-Grocery-Store-Pricing":{"title":"Food Stamps and Grocery Store Pricing","content":"\n## Hastings and Washington\n\nQuestion: How do food stamps affect grocery store pricing?\n\nWhat are the sources of variation in food stamp recipiency?\n\n#### Idea in this paper\n\nNumber of food stamps receipients shopping at a store varies over the month due to when benefits are mailed\n\n* Food stamps are typically arrive at the same time for a large group of people e.g. first of the month\n* Are prices higher when benefits arrive compared to other times of the month within the same store\n\nWhy is this a plausible source of variation?\n\n#### Results\n\n* Benefit households reduce expenditures by 20 percent after the first week of the month. COntinues to decline as the monthw ears on.\n* Non-food expenditures are constant over weeks for benefit households\n\nReferred to as cycling\n\n* Stores increase prices in high poverty areas 2-3 percent in week 1\n* No change in low poverty areas\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Frequentist-Probability":{"title":"Frequentist Probability","content":"\nAssumptions:\n\n1. Repeatable process with uncertain outcomes\n1. Repetitions are independent of each other and identically distributed thus [Independent Identically Distributed](Independent%20Identically%20Distributed.md).\n   1. Distributed means same probability behavior on each repetition\n\n$$P\\_{F}(A \\mid B) = \\lim\\_{n\\to\\infty}\\frac{\\text{\\# of repetitions so far where A = True}}{\\text{\\# of repetitions so far (a.k.a. = n)}}$$\n\nIf the limit exists and is unique, then by the *Law of Large Numbers* it equals the [Classical Probability](Classical%20Probability.md) or [Pascal-Fermat Probability](Pascal-Fermat%20Probability.md) (which I believe are the same?? #edit)\n\nYou have to make a stronger assumption\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Frequentist-Statistics":{"title":"Frequentist Statistics","content":"\nFrequentist statistics is an approach to statistical inference that relies on the frequentist interpretation of probability. In this approach, probability is defined as the long-run frequency of an event occurring in a large number of repeated trials. Frequentist methods focus on estimating parameters of a population from a sample using techniques such as maximum likelihood estimation and hypothesis testing.\n\nIn contrast, Bayesian statistics is an approach to statistical inference that incorporates prior knowledge or beliefs about the parameters of interest into the analysis. Bayesian methods use Bayes' theorem to update prior beliefs based on observed data and produce a posterior distribution of the parameters.\n\nThe key difference between frequentist and Bayesian statistics is the treatment of probability. Frequentist methods rely on the probability of observing data under a null hypothesis, while Bayesian methods incorporate prior beliefs and update them based on observed data to produce a posterior distribution. This difference can lead to different conclusions and interpretations of statistical results, and the choice of approach may depend on the specific research question and available data.\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Gini-Coefficient":{"title":"Gini Coefficient","content":"\nMeasures how uneven a distribution is\n\nLorenz curve\n\nA Lorenz curve is a graph that shows the distribution of income or wealth in a society. It plots the cumulative share of income or wealth held by a certain percentage of the population, starting from the poorest to the richest. The curve compares this distribution with an ideal line of perfect equality where everyone has an equal share. The greater the distance between the Lorenz curve and the ideal line, the more unequal the distribution of income or wealth in that society.\n\n*Econ 201 - Applications of Microeconomics*\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Goodman-Bacon-critique":{"title":"Goodman-Bacon critique","content":"\nFor staggered treament\n\nSuppose treatment is 0-1 indicator\n\n$$\\Large P\\_{jt} = \\alpha + \\gamma_t + \\delta_j + \\beta T\\_{jt} + \\epsilon\\_{jt}$$\n\n* Treatment happens early for some states, later for others\n* The late adopters serve as a control for the ealry adopters, and vice versa\n\n\\#todo 38:00 mins\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Government-Intervention-in-the-Economy":{"title":"Government Intervention in the Economy","content":"\n## Public Policy Analysis\n\n#### When should the government intervene in the economy\n\n[Market Failure](Market%20Failure.md):\n\n* Key result in micro: under certain assumptions, free market delivers efficient outcome\n* Markets first, governments second\n* Under some circumstances markets fail to do so\n* Government intervention can improove efficiency\n\n[Redistribution](Redistribution.md):\n\n* Free market may result in inequality\n* Government reduces inequality through taxes and transfers\n\n1. How might the government intervene\n1. What is te effect of those interventions on economic outcomes\n1. Why do governments choose to intervene in the way that they do\n1. \n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Government-Spending":{"title":"Government Spending","content":"\nDebt sustainability\n\nTaxes\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Greenfield-FDI":{"title":"Greenfield FDI","content":"\nGreenfield FDI refers to a type of foreign direct investment where a company establishes a new business or facility from scratch in a foreign country. This investment involves building new infrastructure, acquiring land, hiring employees and creating a new business entity. It is called \"greenfield\" because the investment is built on undeveloped or \"green\" land. \n\nGreenfield FDI is often considered more risky than other types of FDI since it involves starting from scratch in an unfamiliar market. However, it also offers more control and flexibility for the investing company, as they have complete ownership and control over the new venture. It also provides an opportunity to tailor the business to local conditions and preferences.\n\nGreenfield FDI can bring significant benefits to both the investing company and the host country. For the company, it allows them to expand their operations into new markets, gain access to new customers, and potentially reduce costs through lower labor or resource costs. For the host country, greenfield FDI can bring much-needed investment and jobs, transfer technology and know-how, promote competition and innovation in local markets, and boost economic growth.\n\nOverall, greenfield FDI is an important form of foreign direct investment that can bring significant benefits to both the investing company and the host country if managed effectively.\n\nMost FDI is mergers and acquisitions\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Gross-Domestic-Product":{"title":"Gross Domestic Product","content":"\n* Value of all final goods\n* All value added\n\n---\n\n### Demand Side\n\nMeasured by expenditures, can also be measured by total income, total production or value added\n\n$$\\Large GDP = Y =  C + I + G + NX$$\n\n* C = [Consumption](Consumption.md)\n* I = [Investment](Investment.md)\n* G = [Government Spending](Government%20Spending.md) =  exogenous (taken as given)\n* NX = EX - IM = [Net Exports](Net%20Exports.md)\n\n[Current Account](Current%20Account.md) = NX + NFP\n\nNFP = Net factor payments in macroeconomics refer to the difference between the income earned by domestic factors of production (such as labor and capital) from foreign sources, and the income earned by foreign factors of production from domestic sources. In simpler terms, it is the difference between the total payments received by a country from foreign entities for its resources, and the total payments made by a country to foreign entities for their resources. This factor is included in the calculation of Gross Domestic Product (GDP) to provide a more accurate measure of a country's economic activity.\n\nCapital or labor working for me abroad, factor payent from foreign country to su\n\n50:10 #todo\n\nWe could assume = 0 (generally true for US but not other countries)\n\nBalance of Payments\n\n$$\\Large Y = GNP = GDP + NFP = NI$$\n\n### Supply Side\n\nOutput\n\n$$\\Large Y = F(K,L) * TFP$$\nIf A is in front of the whole equation then = [Total Factor Productivity](Total%20Factor%20Productivity.md)\n\nUnemployment or excess capacity\n\n$$\\Large Y = (K^{\\gamma} * Z^{1-\\gamma})^{\\alpha}*(H*L)^{\\beta} * T^{1-\\alpha-\\beta}$$\nH = Human capital\nZ = intangible capital (measured by R\u0026D for example, another example is branding)\nT = land\n\n#### Capital\n\nCapital = spending money over time, accumulate\n\n$$\\Large K\\_{t+1} = K_t + I_t - \\delta K_t$$\n\ndelta = depreciation\n\n\\#todo 23:00 (powers on the equation)\n\nLog-linearize the equation above: you can a regression\n\nSolow Residual #todo\n\n$$\\Large Y = F(K,L): Y = MPK * K + MPL * L$$\nMarginal Product of Capital = r\n\n**A good approximation**\nCapital share = ⅓\nLabor share = ⅔\n\n#### Factors of Production are paid\n\n* Labor input\n  \n  * paid a wage = w\n  * plus fringe benefits\n* Capital input\n  \n  * paid a interest = r\n  * real interest or rental rate of capital\n* Proprietor income\n\n* Rental income\n\n* Profits\n\n* Net interest\n\n## Real GDP vs Nominal GDP\n\nWhen measuring GDP, it is common to use both chained volume measures and current prices, but the choice between the two depends on the purpose of the analysis and the type of information you want to convey.\n\nChained volume measures, also known as real GDP, adjust for changes in prices over time by using a base year to calculate the value of goods and services produced in a given year. Real GDP is useful for comparing economic growth over time, as it allows us to see how much output has increased or decreased after accounting for changes in prices. This makes it a good measure to use if you want to analyze the long-term trend in the economy.\n\nOn the other hand, current prices, also known as nominal GDP, do not adjust for changes in prices and represent the value of goods and services produced at current market prices. Nominal GDP is useful for analyzing the current state of the economy and the current level of output, as it reflects the prices and quantities of goods and services being produced in the current period. This makes it a good measure to use if you want to analyze the current level of economic activity or the current market value of output.\n\nIn general, if you want to compare economic growth over time or analyze changes in output in real terms, it is appropriate to use chained volume measures. However, if you want to analyze the current state of the economy or the market value of output, it is appropriate to use current prices. It's also worth noting that both measures can provide important insights and can be used together to provide a more complete picture of the economy.\n\n#### Production Approach\n\nNo \"double counting\" in GDP\n\n* Only the final sale of goods and services count\n\nValue Added\n\n* The amount each producer contributes to GDP\n* The revenue generated by each producer minus the value of intermediate products\n\nOnly new production of goods and services counts towards GDP\n\ne.g. House sold many years ago does not count, only commission fee for Realtor\n\nGross National Product (GNP):\n\nMarket value of all the goods and services produced in one year by labor and property supplied by the citizens of a country\n\nRelation of GDP and GNP\n\nGDP = GNP - NFP\nCA = NX + NFP\nGNP = C + I + G + CA\nGDP = C + I + G + NX\nGNP = GDP + NFP = C + I + G + NX + NFP\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Histogram":{"title":"Histogram","content":"\nDensity function:\n\n* How many values that are in the tails\n\nNormal kernel density is better\n\nAd hoc bins and plot the number of \n\nThinking about empirially what the distribution looks like\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Hypothesis":{"title":"Hypothesis","content":"\nAn idea or experiment you want to test and valudate\n\n[Null Hypothesis](Null%20Hypothesis.md)\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Hypothesis-Testing":{"title":"Hypothesis Testing","content":"\nLet's do an example\n\n* Monday = nothing going on = Null Hypothesis = $\\large H_0$\n* Saturday = friends, party = Alternative Hypothesis = $\\large H_A$\n\nOnly by comparing Monday $\\large H_0$ to Saturday $\\large H_A$ you'll know that your Saturday is different and you experienced something new (a nice alternative to a boring Monday)\n\n#### Null Hypothesis\n\n* Something you know\n* You expect nothing to happen\n* You expect no difference\n* With new data you expect nothing new\n\n#### Alternative Hypothesis\n\n* You hope there is something new\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Identifying-Variation":{"title":"Identifying Variation","content":"\nSoaking up all the identifying variation?\n","lastmodified":"2023-05-25T06:37:17.242653372Z","tags":[]},"/notes/Important-Aspects-of-the-Economy-or-Modern-Economy-and-Public-Sector":{"title":"Important Aspects of the Economy or Modern Economy and Public Sector","content":"\n\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Imputation":{"title":"Imputation","content":"\nHow to deal with [Missing Values](Missing%20Values.md)\n\nSpreading the missing values to binary outcomes = linearly interpolating???\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Income-Inequality":{"title":"Income Inequality","content":"\nTrend picture:\n\nDeclining from 1938 to 1978 then rising until 2013\n\nDoes not include taxes or government transfers\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Income-Taxation":{"title":"Income Taxation","content":"\n*How heavily should we tax top income?*\n\n## Tax Policy and the nature of high incomes\n\n#### Income taxes in the U.S.\n\n* The U.S. derived 48% of federal revenue from taxing family income in 2017\n\nCalculation of tax base:\n\n* Start with cash income from all sources.\n* Subtract off adjustments (contributions to health savings accounts and individual retirement accounts, student loan interest, alimony, among others = **Adjusted Gross Income**)\n* **Taxable income** = AGI - (Exemptions + Deductions)\n* Not included in the income definition: Fringe benefits such as health insurance and pension contributions, unrealized capital gains, imputed rent from homeownership\n\n##### Exemptions and Deductions:\n\nExemption: Set amount per household member\nDeductions: Taxpayerchooses one \n\n\\#todo \n\n## Theory\n\n#### Linear Tax\n\nRevenue maximizing rate\n\nz\n\nt - linear tax rate\n\nTax bill:\nT(z) = t\\*z\n\ny = z - T(z) = (1-t)Z\n\n**Marginal tax rate:**\nT'(z) = t\n\n**Average tax rate**\nT/z = t\n\nR = tz(1-t)\n\ndR/dt = Z(1-t) - tdz/d(1-t) = 0\n\n1 - t(1/z) dz/d(1-t) = 0\n\n1 - t/1-t * 1-t/z * dz/d1-t = 0\n\n1 - t/1/t \\epsilon_z = 0\n\nt/1-t \\epsilon_z = 1\n\nt * \\epsilon_z = 1 - t\n\nt + t \\epsilon_z = 1\n\nt = 1/1+/epsilon_z\n\nLaffer Curve:\n\n![laffer curve.png](Image%20Bank/laffer%20curve.png)\n\n![Pasted image 20230420125455.png](Image%20Bank/Pasted%20image%2020230420125455.png)\n\n## Optimal Taxes\n\n#### Linear taxes\n\nSocial Welfare Function\n\n$$\\Large SWF = \\sum_i U_i$$\n\n$$\\Large tz_i = \\text{Tax raised from i}$$\n\n$$\\Large Rev = \\sum_i tz_i$$\nBenefit (B) = Rev/N = (t * sum z_i)/n\n\n![Pasted image 20230420130447.png](Image%20Bank/Pasted%20image%2020230420130447.png)\n\n$$\\Large u(C) = u (z_i -t z_i + t \\bar z) = u((1-t)z_i + t \\bar z)$$\n\n$$\\Large \\Omega = \\sum_i u((1-t)z_i + t \\bar z) $$\n\nAbove ^ everything beyond the sum symbol is maximized utility\n\nd \\omega / dt = 0 = $$\\Large  \\sum_i du/dc (-z_i + \\bar z - t d\\bar z/d(1-t)) $$\n= - \\sum Mu_i Z_i + \\bar z \\sum_i Mu_i - t d \\bar z / d(1-t) \\sum mu_i\n\n## Evidence\n\n---\n\n## Optimal top marginal tax rate\n\n* In practice the tax system is progressive, and t increases with income\n* Deriving full schedule is extraordinarily difficult. Simplify by asking how to set tax at the top tax bracket?\n* Intuition of approach: Increase in marginal tax rate for incomes above z\\* has opposing effects on social welfare:\n  * Marginal cost: Two sources: \n    * \n      1. Less tax revenue from reduced earnings by individuals with incomes in top bracket\n    * \n      2. Reduced utility for high-income (let's assume this is small)\n  * Marginal benefit: Additional tax revenue that can be redistributed\n* Optimal tax rate equals marginal cost with marginal benefit\n\nAssume:\n\n* N individuals above z\\*\n* Denote by $\\large z^m(1-t)$ their average income, which depends on net-of-tax rate 1-t\n* Mechanical (not as a consequence of behavioral response) increase in tax revenue\n  $$\\Large dM = \\[z-z^\\*]dt$$\n* Behavioral response reduces tax revenue:\n  $$\\Large dB = tdz = -t \\frac{dz}{d(1-t)dt} = -\\frac{t}{1-t}\\epsilon_zzdt$$\n* Optimal t such that dM + dB = 0\n  $$\\Large t = \\frac{1}{1+a \\epsilon_Z}$$\n  where $\\Large a = \\frac{z}{z-z^\\*}$\n\na - depends on how high their incomes are on average above z\\*\n\n* Optimal t decreases with $\\Large \\epsilon_Z$ (efficiency)\n* Optimal t decreases with a (thinness of top tail)\n* This tax rate maximizes revenue from the top (places zero weight on impact of taxes on high earner's consumption)\n* Empirically, a $\\large \\approx$ 1.5 #todo\n\n## Empirical Approach\n\n* Assume that e is constant over time and across taxpayers\n* Let $\\large z_0$ be potential income. Then reported income $\\large z = (1-\\tau)^e z_0$\n* Take logs, and including subscript i and t for individual and time period:\n  $$\\Large log(z\\_{it}) = elog(1-\\tau\\_{it}) + Z_0$$\n  * Suggests regressing the log of taxable income on the log of the net-of-tax rate\n  * Problem: not identified using [Ordinary Least Squares](Ordinary%20Least%20Squares.md) because $\\large \\tau$ endogenously determined by z/\n  * Need [Instrumental Variable](Instrumental%20Variable.md) for tax rate. Common approach is to use variation in $\\large \\tau$ arising from tax reform\n\n## Share Analysis\n\n* Consider a tax change only affecting the top 1% of taxpayers, with no change for other levels of income\n* The elasticity of reported income, with $\\large t_0$ and $\\large t_1$ pre- and post-reform years\n  $$\\Large e = \\frac{log(s\\_{t1}) - log(s\\_{t0})}{log(1-\\tau\\_{s,t1}) - log(1-\\tau\\_{s,t_0})}$$\n* where $\\large s_t$ is the share of income earned by the top 1%\n* $\\large \\tau\\_{s,t}$ is the weighted average marginal tax rate faced by this income group\n* Assumption: Share earned by top 1% would be unchanged if not for tax\n* Or could estimate time series regressionL\n  $$\\Large log(s_t) = elog(1-\\tau\\_{st}) + \\epsilon_t$$\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Independence-of":{"title":"Independence of","content":"\n\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Independence-of-Irrelevant-Alternatives":{"title":"Independence of Irrelevant Alternatives","content":"\nAlso known as IIA\n\nIt is an assumption made with a [Multinomial Logistic Regression](Multinomial%20Logistic%20Regression.md) \n\nMultinomial logistic regression is a powerful tool to model choice from a finite set of alternatives, but it comes with an underlying model assumption called the independence of irrelevant alterna- tives, stating that any item added to the set of choices will decrease all other items’ likelihood by an equal fraction.\n\nIndependence across choices\n\nExample:\n\nThe relative preference between choice 2 and choice 3 are not affected by the irrelelvant alternative, choice 1.\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Independent-Identically-Distributed":{"title":"Independent Identically Distributed","content":"\n\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Industry-Concentration":{"title":"Industry Concentration","content":"\n## Firms are getting older and larger\n\nThe Herfindahl-Hirschman Index (HHI) is calculated by adding the squared market shares of all firms in a given industry. The formula for calculating HHI is:\nHHI = ∑(squared market share of each firm)\n\n1:12:00 for how to do the equation #todo\n\n$$\\Large HHI = (\\sum\\_{i} s^2_i) * 10K$$\n\nwhere the squared market share of each firm is calculated by multiplying its market share by itself.\n\nFor example, if there are three firms in an industry with market shares of 40%, 30%, and 20%, the HHI would be calculated as follows:\n\nHHI = (0.4^2 + 0.3^2 + 0.2^2) = 0.34\n\nThe resulting HHI value represents the concentration level of the industry, with higher values indicating greater concentration and lower values indicating less concentration. In general, an HHI value below 1,500 indicates a competitive industry, while a value above 2,500 indicates a highly concentrated industry with potential antitrust concerns. \n\nConsumers could be harmed\n\nWorkers have less choice + non-compete clauses\n\nWhat is the HHI index and how is it calculated\n\nThe Herfindahl-Hirschman Index (HHI) is a measure of market concentration in an industry. It is calculated by adding the squared market shares of all firms in a given industry. The formula for calculating HHI is: HHI = ∑(squared market share of each firm). The resulting HHI value represents the concentration level of the industry, with higher values indicating greater concentration and potential antitrust concerns. In general, an HHI value below 1,500 indicates a competitive industry, while a value above 2,500 indicates a highly concentrated industry.\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Infinity":{"title":"Infinity","content":"\nInfinity / (infinity + 1) = 1 / 1 = 1\n\nAnother way to understand why infinity/infinity+1 = 1 is to consider that infinity is not a specific number, but rather a concept that represents something unbounded or without limit. When we say that infinity/infinity+1 is equal to 1, what we mean is that as the value in the numerator and denominator get larger and larger without bound, the ratio approaches 1. This is because adding 1 to infinity does not significantly affect its value when compared to infinity itself, so the ratio of infinity to infinity+1 is essentially the same as the ratio of infinity to infinity.\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Instrumental-Variable":{"title":"Instrumental Variable","content":"\nStatistically same method as RCT/RED, but ex-post from the researchers perspective\n\n\"Perfect compliance\" case if the treatment itself was randomly assigned so the instrument is the treatment\n\ne.g. winning the lottery and bankruptcy\n\n\"Imperfect compliance\" if assignment to treatment only predicts actual treatment and then you could reweight it using [2SLS](2SLS.md)\n\nReweight by e.g. compliance rate\nA Quick way that will produce wrong standard errors:\n\n* Reduced form estimate and divided by compliance\n\nMatrix form?\n\n![Screen Shot 2022-02-04 at 10.33.28 PM.png](Image%20Bank/Screen%20Shot%202022-02-04%20at%2010.33.28%20PM.png)\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Interaction-Terms":{"title":"Interaction Terms","content":"\nThese capture the fact that a characteristic such as *distance to public transit* might affect rental price differently depending on e.g. *rural vs. urban*\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Intergenerational-Mobility":{"title":"Intergenerational Mobility","content":"\nRank-Rank slope\n\nUS = 0.341\nDenmark = 0.180\n\n[Quantile Regression](Quantile%20Regression.md)\n\nCorrelation is the strongest at the bottom of the distribution\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Investment":{"title":"Investment","content":"\nWhat determines investment:\n\n* r = real interest rate = i - $\\pi$ (approximation)\n  \n  * (1+r)(1+$\\pi$) = 1 + i (true equation)\n  * Cheap to borrow = invest more\n* uncertainty about demand\n  \n  * equilibrium but not a good one\n  * Uncertainty up = investment down\n  * High uncertainty = consumption of durable goods really gets affected\n\nIncludes inventory\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Iteratively-Reweighted-Least-Squares":{"title":"Iteratively Reweighted Least Squares","content":"\nIRWLS\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/LASSO":{"title":"LASSO","content":"\nBiased\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/LOESS-Curve":{"title":"LOESS Curve","content":"\n\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/LaTeX":{"title":"LaTeX","content":"\n## Bayesian Statistics:\n\n---\n\nBackground Information - $\\mathcal{B}$\n\n\\\\includegrpahics{}\n\ntilde = \\\\sim\n\nstar = \\\\ast\n\n## Graphs\n\n\\\\begin{figure}\n\\\\centering\n...\n\\\\includegraphics\\[scale=0.9]{stat-206-quiz-3-figure-for-part-(f).pdf}\n...\n\\\\end{figure}\n\n## Partial Derivative\n\n\\\\frac{\\partial f}{\\partial x_i}\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Labor-Share-of-Output":{"title":"Labor Share of Output","content":"\nFalling from 1975 to 2015\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Law-of-Iterated-Expectations":{"title":"Law of Iterated Expectations","content":"\nThe law of iterated expectations is a way of understanding that the [Probability](Probability.md) of something happening is based on the probability of something else happening. It's like a chain reaction!\n\nFor example, if you have a bowl of mixed candy, the probability of you picking a blue candy is based on the probability of you picking any candy from the bowl. The probability of picking any candy is based on the probability of you reaching into the bowl.\n\nSo the law of iterated expectations is like a chain reaction - one thing happening is based on the probability of something else happening.\n\nEnglish: The law of iterated expectations is a way of understanding that the probability of something happening is based on the probability of something else happening. It's like a chain reaction, where one thing happening is based on the probability of something else happening. For example, the probability of picking a blue candy from a bowl of mixed candy is based on the probability of picking any candy from the bowl, which is based on the probability of reaching into the bowl.\n\nConditional variance is a measure of how much the variance of a random variable changes when the values of another random variable are taken into account. It is used to show how the expectation of one random variable depends on the values of another random variable. For example, if we have a bowl of candy with different colors, the probability of picking a blue candy changes depending on the probability of picking any candy from the bowl. The conditional variance is a measure of how much the variance changes when the probability of picking any candy from the bowl is taken into account.\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Linear-Algebra":{"title":"Linear Algebra","content":"\nGoing backwards is like an inverse matrix when desired output is being multipled then the vector it came from comes out\n\n3blue1 brown essence of linear algebra\n\n[Markov Matrix](Markov%20Matrix.md)\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Linear-Regression-Model":{"title":"Linear Regression Model","content":"\n[Ordinary Least Squares](Ordinary%20Least%20Squares.md)\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Log-function":{"title":"Log function","content":"\nWhen taking the log it is because the values have a large range\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Logical-Internal-Consistency":{"title":"Logical Internal Consistency","content":"\nLogical internal consistency refers to the logical coherence and consistency of ideas, arguments, or statements within a particular system or framework. It involves ensuring that all the elements within a system or argument are logically connected and do not contradict each other. In other words, there should be no logical fallacies or inconsistencies within the system or argument. Logical internal consistency is important in ensuring that ideas and arguments are sound and credible. \n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Logistic-Regression":{"title":"Logistic Regression","content":"\nAlso known as: Logit\n\nEstimates by how they affect the odds ratio\n\nTime Varying Logit? \n\n* time-series analysis\n\n[Pseudo R-squared](Pseudo%20R-squared.md)\n\n[z-value](z-value.md)\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/MCM":{"title":"MCM","content":"\n\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Machine-Learning":{"title":"Machine Learning","content":"\n[Big Data](Big%20Data.md)\n\n[Odds and Probabilities](Odds%20and%20Probabilities.md)\n\n[LASSO](LASSO.md)\n\n[Resampling](Resampling.md)\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Macroeconomic-Variables":{"title":"Macroeconomic Variables","content":"\n|Flow|Stock|\n|----|-----|\n|Inflow \u003e outflow|Stock accumulates|\n|||\n\nrate * stock = flow\n\n### Stocks\n\nMeasured at a point of time\n\nEmployment\n\nCapital Stock\nLabor\n\nMoney Supply\n\nexm\n\nWhat are examples of variables that are stocks in economics?\n\nIn economics, stocks refer to variables that accumulate over time and represent a quantity at a particular point in time. Examples of macroeconomic variables that are stocks include:\n\n1. Capital stock: This represents the total amount of physical capital (machinery, equipment, buildings, etc.) available in an economy at a given point in time.\n\n1. National wealth: This refers to the total value of all assets owned by individuals, businesses, and the government in an economy at a particular point in time.\n\n1. Foreign reserves: This represents the stock of foreign currency and other liquid assets held by a country's central bank to support its currency and maintain economic stability.\n\n1. Money supply: This refers to the total amount of money circulating in an economy at a given point in time, including cash, bank deposits, and other forms of money.\n\n1. Human capital: This represents the stock of knowledge, skills, and abilities possessed by individuals in an economy at a particular point in time.\n\n### Flows\n\nLabor\nExports\nImports\nConsumption\nSavings\nInvestment\nGDP or Y\n\nImports and Exports\n\nMeasured over a period of time\n\nWhat are examples of variables that are stocks in economics 1. Gross Domestic Product (GDP)\n2. Imports and exports\n3. Investment\n4. Consumption expenditure\n5. Government spending\n6. Savings\n7. Net income from abroad\n8. Disposable income\n9. National debt\n10. Capital formation\n\n### Rates / Prices\n\nr = real interest rate\ni = nominal interest rate\n$\\pi$ = inflation\nU = unemployment rate\nE = exchange rate\n$\\Large \\delta$ = depreciation rate\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Macroeconomics":{"title":"Macroeconomics","content":"\nHow systems work at the country level and global economy\n\n### Personal finance/decisions:\n\n[Macroeconomic Variables](Macroeconomic%20Variables.md)\n\n* r = real interest rate\n  * r\\* = foreign exchange rate\n* i = nominal interest rate\n* $\\pi$ = inflation\n* U = unemployment rate\n* E = exchange rate\n* G = government expenditure, T = taxes, D = debt\n\n### Business decisions:\n\n* Economic growth (g)\n  * I = investing\n\n### Interpreting news\n\n### Voting\n\n### Global\n\nImport\nExports\n\nSpillovers = bigger as the countries become more integrated\n\nRussia invasion of Ukraine:\n\n* European Energy market\n* Fertilizer \u0026 Grains\n\n#### Outsourcing:\n\nBusiness -\u003e Global\n\n[Consumption](Consumption.md)\n\n[National Income Accounts](National%20Income%20Accounts.md)\n\n[Solow Model](Solow%20Model.md)\n\n[Economic Growth](Economic%20Growth.md)\n\n[Capital Account Liberalization and the Neoclassical Growth model](Capital%20Account%20Liberalization%20and%20the%20Neoclassical%20Growth%20model.md)\n\nGDP-indexed bonds #todo\n\nGDP is low = low interest rate and vice versa\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Mandated-Benefits":{"title":"Mandated Benefits","content":"\n* Tempting to view mandates as additional taxes on firms and apply same analysis as above\n  \\#todo \n\n#### Graphs\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Market-Failure":{"title":"Market Failure","content":"\nCorrecting market failure\n\n## Externalities\n\nOne agent's actions affect others' utility.\n\nSome goods are unpriced - actions do not have rewards or costs that reflect benefit or harm to society.\n\n#### Positive Externality\n\nVaccinations help others, but lack of reward fails to incentivize getting vaccinated.\n\n#### Negative Externality\n\nPollution emissions by firms do harm to human health and physical property, yet firms do not incur a cost from polluting.\n\n### How to Intervene?\n\n* Create a price for the action.\n  * e.g. (taxes on cigarettes, gasoline, carbon, subsidies for vaccinations, research and development)\n* Dictate the desired action to agents.\n  * e.g. (fuel economy standard, pollution limits, require students to be vaccinated to attend school)\n\n## Public Goods\n\nPublic goods: Common benefit, private cost\n\nTragedy of the commons\n\nNon-rivalrous\n\n* Free-rider problem\n* Achieving efficient outcome requires organization to coordinate the actions of individuals\n\n### How to Intervene?\n\n* Direct provision of the public good\n  \n  * e.g. building highways, provide national defense and policing, operate national parks\n* Subsidize provision of public good\n\n## Asymmetric Information and missing markets\n\nMarkets fail when information differs between buyers and sellers\n\n#### Insurance: Health, life, car etc\n\n* Buyer of health insurance has information about how healthy\n* Leads to [Adverse Selection](Adverse%20Selection.md). Only the (unobserved to the insurer) sickest buy insraunce\n* Government provision, or mandated coverage, may be policy responses\n  * avoids [Adverse Selection](Adverse%20Selection.md)\n\n#### Credit constraints and education\n\n* Borrowing against future income may be limited. (Incentive to default)\n\n### Individual failures\n\nBehavorial economics: What about when individual do not seem to act in their own best interests? (e.g. insufficient savings for retirement)\n\nControversial!\n\nTwo views:\n\n* Individual failures do not exist or difficult to identify\n\n* Libertarian view. Individual failures exist and should be addressed. Cognitive limitatoins, self control problems.\n  \n  * This is the view of behavorial economics\n  * \n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Markov-Matrix":{"title":"Markov Matrix","content":"\nColumns sum to 1\nNo negative values\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Matrix":{"title":"Matrix","content":"\nExplain simply what a matrix is and give an example. also, tell me how it relates to data and how it can be used in languages like R and Python. Be simple but thorough.\n\nA matrix is a rectangular arrangement of numbers or symbols in rows and columns. It is used to represent data that has multiple attributes or variables. For example, a matrix could be used to represent the grades of students in a class, where each row represents a student and each column represents a subject.\n\nMatrices are commonly used in data analysis and machine learning because they allow us to organize and manipulate large amounts of data efficiently. In programming languages like R and Python, matrices are represented as arrays, which are collections of values indexed by one or more indices.\n\nIn R, you can create a matrix using the `matrix()` function. For example, the following code creates a 3x3 matrix with random numbers between 1 and 10:\n\n````r\nmy_matrix \u003c- matrix(data = runif(9, min = 1, max = 10), nrow = 3, ncol = 3)\n````\n\nIn Python, you can create a matrix using the NumPy library. For example, the following code creates a 3x3 matrix with random numbers between 1 and 10:\n\n````python\nimport numpy as np\n\nmy_matrix = np.random.randint(low=1, high=10, size=(3,3))\n````\n\nOnce you have created a matrix in R or Python, you can perform various operations on it such as addition, subtraction, multiplication, transpose etc. These operations are useful for performing calculations on large datasets efficiently.\n\nOverall matrices play an important role in representing data in an organized manner which makes it easier for data analysis purposes.\n\n## Matrices and Vectors\n\n[Vector](Vector.md)\n\nExplain how matrices and vectors are related especially in terms of data and programming languages like R and Python\n\nMatrices and vectors are related in that a vector can be considered as a one-dimensional matrix, or a matrix with only one row or one column. In data analysis and machine learning, vectors are often used to represent a single observation or data point.\n\nIn programming languages like R and Python, vectors can be created using the `c()` function in R and the NumPy library in Python. For example, the following code creates a vector of heights in inches:\n\n````\n# R\nheights \u003c- c(65, 68, 72, 70)\n\n# Python\nimport numpy as np\n\nheights = np.array([65, 68, 72, 70])\n````\n\nMatrices can be created by combining multiple vectors together. For example, the following code creates a matrix of heights and weights for four individuals:\n\n````\n# R\nheights \u003c- c(65, 68, 72, 70)\nweights \u003c- c(150, 170, 180, 160)\ndata_matrix \u003c- matrix(data = c(heights, weights), nrow = 4)\n\n# Python\nimport numpy as np\n\nheights = np.array([65, 68, 72, 70])\nweights = np.array([150, 170, 180, 160])\ndata_matrix = np.column_stack((heights, weights))\n````\n\nOnce you have created a matrix or vector in R or Python you can perform various calculations on them such as addition and multiplication. Matrices are especially useful when working with large amounts of data since they allow for efficient manipulation and analysis.\n\nOverall matrices and vectors are important tools for representing and analyzing data in programming languages like R and Python.\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Maximum-Likelihood":{"title":"Maximum Likelihood","content":"\nMaximum Likelihood\n\nWithout the i.i.d assumption we can't write it as a product\n\nWithout i.i.d you can still use maximum likelihood but it is more complicated\n\nLikelihood function: $$L (\\beta) = f\\_{y|X}$$\n\nMaximum likelihood estimator for a certain $\\beta_0$ maximizes the probability of obtaining the data you have\n\nSecond derivative of the log lieklihood function at the maximum likedlihood estiamte\n\nMaximum Likelihood can be used in both [Bayesian Statistics](Bayesian%20Statistics.md) and [Frequentist Statistics](Frequentist%20Statistics.md)\n\nThere is an arbitrary constant that means we can change the curve up and down without changing the shape\n\nFIsher thought of it as a frequentist inferential method\n\nAlso approximate Bayesian method\n\nStandard error is the square root of the variance\n\nEstimated SE is the estimated square root of the variance of the random variable\n\nVariance scale \n\n$\\Large y_i$ = family income\n\nSD would be in dollars \\\u003c- speical case \n\n* Same as data scale\n\nVariance would be in dollars \n\n* (data scale)$^2$ \n\n![Screenshot 2023-02-08 at 12.14.44 PM.png](Image%20Bank/Screenshot%202023-02-08%20at%2012.14.44%20PM.png)\n\nRed line = n = 403/2 (less data)\nGreen line = n = 403 (more data)\n\nSecond derivative of these log likelihood functions at theta_MLE would be negative\n\nThus we would multiply it by -1\n\nSecond derivative = strength of curvature\n\nAs n goes up the abdsolute value of the 2nd derivative would be gets bigger\n\nas n goe sup then -1 * second derivative would also go up\\*\n\nAs n increases information in y about theta increases\n\n[Observed Information](Observed%20Information.md)\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/McFadden-R-squared":{"title":"McFadden R-squared","content":"\nMcFadden's R-squared is a measure of the goodness of fit of a logistic regression model. It is a commonly used metric to evaluate how well the model fits the data. McFadden's R-squared is based on the ratio of the log-likelihoods of the fitted model and a baseline model with only an intercept term. The formula for McFadden's R-squared is:\n\n1 - (log-likelihood of the fitted model / log-likelihood of the null model)\n\nwhere the null model is a logistic regression model with only an intercept term. McFadden's R-squared ranges from 0 to 1, with higher values indicating a better fit of the model to the data. It is worth noting that McFadden's R-squared is not an actual coefficient of determination (like the R-squared in linear regression), but rather a pseudo-R-squared, which provides an indication of the proportion of the variance in the dependent variable explained by the independent variables in the model.\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Missing-Completely-At-Random":{"title":"Missing Completely At Random","content":"\nMCAR\n\nThis model assumes that the missing values are also an [Independent Identically Distributed](Independent%20Identically%20Distributed.md) *Random Sample* from the population of interest\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Missing-Values":{"title":"Missing Values","content":"\nOne way to deal with this is called:\n\n[Missing Completely At Random](Missing%20Completely%20At%20Random.md)\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Modern-Economy-and-Public-Sector":{"title":"Modern Economy and Public Sector","content":"\n### Important Aspects of the Economy\n\n* Rising [Income Inequality](Income%20Inequality.md)\n* Reduction in the share of income going to labor: [Labor Share of Output](Labor%20Share%20of%20Output.md)\n* [Changes in the Labor Market](Changes%20in%20the%20Labor%20Market.md): automation and job polarization\n* Response of workers to the changes: [Education](Education.md) and [Intergenerational Mobility](Intergenerational%20Mobility.md) | [Spatial Mobility](Spatial%20Mobility.md)\n* Increasing [Industry Concentration](Industry%20Concentration.md)\n* \n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Multinomial-Logistic-Regression":{"title":"Multinomial Logistic Regression","content":"\nWhen thinking about utility we can use the equation below.\n\nThe utility from agent $i$ from choice $j$ is given by the following:\n\n$$y^\\**{ij} = x^\\prime*{i}\\beta\\_{0j} + \\epsilon\\_{ij}$$\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Multivariate-Discrete-Choice-Models":{"title":"Multivariate Discrete Choice Models","content":"\nHere we can think mainly of [Multinomial Logistic Regression](Multinomial%20Logistic%20Regression.md) \n\nThese types of models attempt to describe, explain two or more discrete choices\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Nap":{"title":"Nap","content":"\nis it healthy to sleep for 2 hours during the day?\n\nAs an AI language model, I do not have personal opinions or experiences. However, according to sleep experts, taking a short nap during the day can be beneficial for some people. A 20-30 minute nap can help improve alertness, concentration, and mood. However, sleeping for 2 hours during the day may interfere with nighttime sleep and lead to feeling groggy or disoriented upon waking up. It is important to listen to your body and determine what works best for you in terms of nap length and timing. \n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/National-Income-Accounts":{"title":"National Income Accounts","content":"\nNational Income\n\nBalance of payment\n\nHow are these two topics related?\n\nNational income and balance of payment are related because they both provide information on the economic performance of a country. National income accounts measure the total value of goods and services produced in a country over a specific period, while balance of payment accounts measure the transactions between a country and other countries in terms of trade, investment, and financial flows. The balance of payment is influenced by national income as higher national income leads to higher imports, exports, and investments. Therefore, understanding national income provides insight into a country's ability to generate foreign exchange and its impact on the balance of payments. Similarly, analyzing the balance of payments helps to understand how a country's economic activity affects its national income.\n\nGive me a bullet point list of what is included under balance of payments in macroeconomics• Current account\n• Capital account\n• Financial account\n• Official reserve account \n• Errors and omissions\n\nGive me a list of overlap between national income and balance of payments and a list of what is distinctively differentOverlap between national income and balance of payments:\n\n1. Both national income and balance of payments are used to measure the economic performance of a country.\n1. They both involve the use of statistics to measure various economic indicators.\n1. National income and balance of payments are interconnected as they both impact a country's overall economic activity.\n\nDistinctively different between national income and balance of payments:\n\n1. National income accounts focus on measuring the production and income generated by a country's residents, while balance of payments measures a country's transactions with the rest of the world.\n1. National income is concerned with production within a country's borders, while balance of payments includes transactions that occur beyond a country's borders.\n1. National income accounts primarily measure gross domestic product (GDP), while balance of payments includes measurements such as exports, imports, foreign investment, and remittances.\n1. National income accounts are typically used for domestic policy-making, while balance of payments is more relevant for international trade and monetary policy-making.\n\nIn 1-2 sentences how would you explain to someone how national income and balance of payments are related\n\nNational income accounts and balance of payments are related as national income accounts record a country's income and expenditure, while balance of payments records its transactions with other countries.\n\nGDP vs. GNP\n\n[Gross Domestic Product](Gross%20Domestic%20Product.md)\n\nGDP = produced in a country's borders? #todo\n\nGNP = produced by country's nationals\n\n* Cross-border transfers of profits\n\nGNP = National Income (NI)\n\nUS difference is 2-3%\n\nFRED as data source\n\nWhich accounts are national income accountsNational income accounts refer to a set of economic indicators and measures that are used to track the performance of a country's economy. These accounts include:\n\n1. Gross Domestic Product (GDP): This is the total value of all goods and services produced within a country's borders in a given period.\n\n1. Gross National Product (GNP): This is the total value of all goods and services produced by a country's citizens, regardless of their location, in a given period.\n\n1. Net National Product (NNP): This is the GNP minus depreciation, which represents the wear and tear on capital goods during production.\n\n1. National Income (NI): This is the total income earned by individuals and businesses within a country in a given period.\n\n1. Personal Income (PI): This is the income received by individuals from all sources, including wages, salaries, rents, interest, and dividends.\n\n1. Disposable Personal Income (DPI): This is the income available to individuals after taxes have been paid.\n\nThese accounts are used by governments, policymakers, economists, and investors to understand the state of an economy and make informed decisions about fiscal policy, monetary policy, and investment strategies.\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Nearest-Neighbor-Matching":{"title":"Nearest Neighbor Matching","content":"\nNearest neighbor matching explain\n\nNearest neighbor matching is a statistical technique used in observational studies to match individuals or groups based on their similarity. The goal of this method is to reduce bias and improve the accuracy of the study results by comparing similar subjects.\n\nIn nearest neighbor matching, each subject in the treatment group is matched with the most similar subject in the control group. Similarity is determined based on a set of predefined characteristics or variables, such as age, gender, education level, and income. Once the matches are made, the treatment and control groups are compared based on their outcomes.\n\nThis method is particularly useful when it’s not possible to conduct a randomized controlled trial due to ethical or practical reasons. However, it’s important to note that nearest neighbor matching can still be affected by confounding variables and selection bias if not properly implemented.\n\nOverall, nearest neighbor matching can be a useful tool for researchers looking to reduce bias and improve the accuracy of observational studies.\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Net-Exports":{"title":"Net Exports","content":"\n### What determines:\n\n#### Exports:\n\n* Real exchange rate\n* Real wages\n* Comparative Advantage\n* Foreign Demand\n* Domestic Supply\n* Trade Barriers\n\n#### Imports\n\n* Real exchange rate\n* Comparative Advantage\n* Local Demand\n* Foreign Supply\n* Trade Barriers\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/New-Information":{"title":"New Information","content":"\n\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Neyman-Style-Confidence-Intervals":{"title":"Neyman Style Confidence Intervals","content":"\nLarge sample confidence intervals\nStemming from the [Central Limit Theorem](Central%20Limit%20Theorem.md)\n\nLots of instantiations of the sample\n\nFrequentist probability statement and make it look like a Bayesian\n\nLarge sampel approach\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Normative":{"title":"Normative","content":"\nWhat we should do\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Null-Hypothesis":{"title":"Null Hypothesis","content":"\nA null hypothesis is a statement that states that there is no difference between two groups or sets of data. It is used in scientific experiments and research to determine whether a certain outcome is due to chance or if there is a real difference between the groups.\n\nThink of it like this: if you have two groups of people, one group with a certain type of trait and another group without that trait, and you want to test whether there is a difference between the two groups, you can use a null hypothesis. The null hypothesis states that there is no difference between the two groups, and if the results of the experiment show that there is a difference, then it means that the null hypothesis is incorrect and the difference is real.\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Observed-Information":{"title":"Observed Information","content":"\n\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Odds":{"title":"Odds","content":"\nOdds is the probability of the event occurring divided by the probability of the event not occurring\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Odds-and-Probabilities":{"title":"Odds and Probabilities","content":"\n[Odds](Odds.md) and probabilities are two different ways of expressing the likelihood or chance of an event occurring.\n\nProbability is a measure of the likelihood of an event occurring, expressed as a number between 0 and 1. A probability of 0 means that the event will not occur, while a probability of 1 means that the event is certain to occur. For example, if we toss a fair coin, the probability of getting heads is 0.5 or 50%, since there is an equal chance of getting heads or tails.\n\nOdds, on the other hand, is a ratio that compares the number of ways an event can occur to the number of ways it cannot occur. Odds can be expressed in several different ways, such as \"3 to 1\" or \"3:1\" or \"3/1\". For example, if the odds of winning a particular lottery are 1 in 10,000, this can also be expressed as odds of 1/9999, which means that there is one way to win and 9999 ways to lose.\n\nTo convert from probability to odds, we can use the following formula:\n\nodds = probability / (1 - probability)\n\nFor example, if the probability of an event occurring is 0.7, then the odds are:\n\nodds = 0.7 / (1 - 0.7) = 0.7 / 0.3 = 7/3\n\nTo convert from odds to probability, we can use the following formula:\n\nprobability = odds / (odds + 1)\n\nFor example, if the odds of an event occurring are 5 to 1, then the probability is:\n\nprobability = 5 / (5 + 1) = 5/6\n\nSo, in summary, while probabilities measure the likelihood of an event occurring as a number between 0 and 1, odds express the likelihood of an event occurring as a ratio between the number of ways an event can occur to the number of ways it cannot occur.\n","lastmodified":"2023-05-25T06:37:17.382655539Z","tags":[]},"/notes/Omitted-Variable-Bias":{"title":"Omitted Variable Bias","content":"\n\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/One-Hot-Encoding":{"title":"One-Hot Encoding","content":"\nOne-hot encoding is a technique used to convert categorical variables into a format that can be easily understood by machine learning algorithms. It involves representing each category as a binary vector, where each element in the vector corresponds to a specific category and takes on either a 0 or 1 value. \n\nFor example, let's say we have a dataset of fruit types, which includes the categories \"apple,\" \"banana,\" and \"orange.\" To use this data in a machine learning model, we could one-hot encode the fruit types as follows:\n\n* Apple: \\[1, 0, 0]\n* Banana: \\[0, 1, 0]\n* Orange: \\[0, 0, 1]\n\nIn this encoding scheme, each fruit type is represented by a vector with three elements. The first element corresponds to \"apple,\" the second element to \"banana,\" and the third element to \"orange.\" The value of each element in the vector is either 0 or 1 depending on whether or not that particular fruit type is present in the data.\n\nOne-hot encoding can be particularly useful when dealing with categorical variables that have no inherent order or hierarchy (e.g., colors, gender). By converting these variables into numerical vectors, we can incorporate them into our machine learning models and make predictions based on their values. \n\nDummy variables\nIndicator variables\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Ordinary-Least-Squares":{"title":"Ordinary Least Squares","content":"\nThe estimator assumes:\n\n* Linear relationship\n* Regressors are linearly independent\n  * X has full column rank\n* Spherical errors\n* x and u are uncorrelated: $$\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Outliers":{"title":"Outliers","content":"\nObservations that fall beyond the overall cluster of data\n\n## Coding\n\n````r\n# run OLS regression\nm \u003c- lm(outcome ~ predictor, data = d)\n\n# make residual diagnostics\nlibrary(olsrr)\nols_plot_resid_lev(m)\n````\n\nFinds observations that are obvious outliers\n\n![Screenshot 2023-05-07 at 7.31.41 PM.png](Image%20Bank/Screenshot%202023-05-07%20at%207.31.41%20PM.png)\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/P-Value":{"title":"P-Value","content":"\nP-value is a **measure of similarity between things** or a probability of similarity\n\n* High p-value = things are similar\n* Low p-value = things are different\n\nWhen are things **significantly different? Below 5%** of similarity\nFor example:\n\n* when the similarity of your days is below 5% (p-value \\\u003c 0.05), your days are significantly different\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Panel-Study-of-Income-Dynamics":{"title":"Panel Study of Income Dynamics","content":"\nPay attention to..\n\nCardinal vs ordinal\n\n* Coding of categorical variables (race , sex, etc.)\n* Coding of continuous variables\n  * Top coding\n  * Missing values\n\n[Presenting Data](Presenting%20Data.md)\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Pascal-Fermat-Probability":{"title":"Pascal-Fermat Probability","content":"\nCommon sense approach\n\nBasic idea: Model of the world a.k.a [Equally-Likely Model](Equally-Likely%20Model.md)\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Permanent-Income-Hypothesis":{"title":"Permanent Income Hypothesis","content":"\nConsider an individual who lives for T periods with lifetime utility\n\nConcave function\n\n$$\\Large U = \\sum\\_{t=0}^{T} u (Ct), with u'(\\cdot), u''(\\cdot)$$\n\nLifetime consumption should be smaller or equal to  than lifetime income\n\n## Maximization Problem\n\n$$\\Large \\mathcal{L} = \\sum\\_{t=1}^{T} u (Ct) $$\n\n$$\\Large u'(C_t) = \\lambda$$\n\nThis equation means consumption (no subscript on lambda) is constant over time. Then we have:\n\n$$\\Large \\sum\\_{t=1}^{T} (Ct) = TC_t$$\n\n## Implications\n\nImpact of Permanent Income and Transitory Income\n\nTransitory Income: $$\\Large Yt - \\frac{1}{T}(A_0 + \\sum\\_{t=1}^{T} Y_t)$$\n\nImpact of Permanent Income on Savings\n\n$$\\Large S_t = Y_t - C_t = Y_t - \\frac{1}{T}(A_0 + \\sum\\_{t=1}^{T} Y_t) $$\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Poisson-Distribution":{"title":"Poisson Distribution","content":"\nOften used for count data?\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Positive":{"title":"Positive","content":"\nFactual\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Posterior-Probability":{"title":"Posterior Probability","content":"\n\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Presenting-Data":{"title":"Presenting Data","content":"\nSummary stats\n\nLabel your variables\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Prior-Probability":{"title":"Prior Probability","content":"\n\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Probability":{"title":"Probability","content":"\nA [Probability Model](Probability%20Model.md) is going from the population to the random sample\n\nKind of the opposite direction of inference\n\nThinking about $s$ rather than $\\theta$. \n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Probability-Density-Function":{"title":"Probability Density Function","content":"\nAlso known as PDF\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Probability-Measures":{"title":"Probability Measures","content":"\nMeasures with a total mass = 1\n\n$\\Omega$ - omega, defines the sample space\n\n* Could be any set because we can put the outcomes of a random experiment into it\n\nCould be a rectangle, where are = 1\n\nProbability to get any outcome = 1\n\n$\\mathbb{p}$ maps subsets to numbers\n\nWe want:\n\n$\\Large \\mathbb{P}(\\Omega) = 1$\n\n$\\Large \\mathbb{P}(A) \\space \\epsilon \\space \\[0,1]$\n\nThus no matter what subset (A) we choose, the area we measure should be between 0 and 1.\n\n$\\mathbb{P}: A \\rightarrow \\mathbb{R}$\n\nA = collection of subsets\n\nIf subsets A, B are disjoint (i.e. no overlap in area) \n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Probability-Model":{"title":"Probability Model","content":"\nThe formula for [Standard Errors](Standard%20Errors.md):\n\nsquare root(thetahat(1-thetahat)/n)\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Probability-Theory":{"title":"Probability Theory","content":"\n[Probability Measures](Probability%20Measures.md)\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Propensity-score-reweighting":{"title":"Propensity-score reweighting","content":"\nPropensity-score reweighting is a statistical technique used in economics to control for observables, which are the characteristics of individuals or groups that can affect the outcome of an experiment or study. The technique involves creating a propensity score, which is the probability of an individual or group being assigned to a treatment group based on their observable characteristics.\n\nThe propensity score is then used to weight the data so that it reflects the distribution of observable characteristics in the treatment and control groups. This creates a weighted sample that is more representative of the population being studied and reduces bias in estimating treatment effects.\n\nFor example, in a study comparing the effectiveness of two different job training programs, there may be differences between the treatment and control groups in terms of age, education level, and work experience. By using propensity-score reweighting, researchers can ensure that these differences are accounted for and that any observed differences in outcomes between the two groups are due to the treatment itself rather than other factors.\n\nOverall, propensity-score reweighting is a powerful tool for controlling for observables in economics research and can help improve the accuracy and reliability of experimental results.\n\n## Steps to use this method\n\n1. Define the treatment variable: Identify the variable that represents the treatment or intervention of interest, such as receiving a certain medication or participating in a particular program.\n\n1. Determine covariates: Select a set of covariates that may affect both treatment assignment and outcome. These variables should be measured for all individuals in the sample and include demographic characteristics, health status, education level, income, etc.\n\n1. Estimate propensity score: Use statistical methods to estimate the probability of receiving treatment (propensity score) based on the covariates identified in step 2.\n\n1. Check balance: Evaluate whether the estimated propensity scores result in balanced treatment and control groups with similar distributions of covariates.\n\n1. Calculate weights: Use the inverse of each individual's estimated propensity score to calculate a weight for each observation in the sample.\n\n1. Apply weights: Apply the calculated weights to regression models to adjust for differences in covariate distributions between treatment and control groups.\n\n1. Check results: Evaluate whether the reweighted regression results provide more accurate estimates of treatment effects than unadjusted models by comparing effect sizes and statistical significance levels.\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Pseudo-R-squared":{"title":"Pseudo R-squared","content":"\nin [R](R.md):\n\n````r\n1 - model$deviance / model$null.deviance\n````\n\n[McFadden R-squared](McFadden%20R-squared.md)\n\nhttps://stats.stackexchange.com/questions/8511/how-to-calculate-pseudo-r2-from-rs-logistic-regression #todo \n\nhttps://search.r-project.org/CRAN/refmans/DescTools/html/PseudoR2.html\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Public-Policy":{"title":"Public Policy","content":"\nIncorporate the following terms always: trade-off, incentives, marginal people, over or under allocating, equality, internalities, peer effects and spillovers\n\n**New York 2014 ban on hydraulic fracturing: the practice of using fracking techniques to drill/extract shale oil and gas is banned within the state of NY**. A. \\[50%] Analyze the four E's for this policy (Effective, Efficient, Equitable, Electable). B. \\[25%] Who likely wins and loses from the policy? C. \\[25%] What are some potential unintended or secondary consequences from the policy? - Throughout, be sure to note any key assumptions you make in your analysis. Incorporate the following terms always: trade-off, incentives, marginal people, over or under allocating, equality, internalities, peer effects and spillovers.\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Python":{"title":"Python","content":"\n## Statistical Models\n\n#### Linear Regression model\n\nStatsmodels package can be used for linear regression in python\n\n````python\n\nimport statsmodels.api\n\nX = df.var1\nY = df.var2\n\n# make sure it does not \nX = sm.add_constant(X)\n\n# run the actual regression\nlm_fit = sm.OLS(Y,X).fit()\n\n# print output\nprint(lm_fit.summary())\n\n# print as latex\nprint(lm_fit.summary().as_latex())\n\n\nlm_fit. # will get\n\n# predictions\npred = lm_fit.fittedvalues\n````\n\n#### Autocorrelation\n\n$$\\Large y_t = \\alpha + \\beta y\\_{t-1} + \\epsilon_t$$\nStandard regression formula that assumes homoskedacity\n\nTo check:\n\nwhether the beta is 0 or not euqla to zero\n\nyt = r\n\nThe plot in lecture 4 econ294a shows that there is autocorrelation\n\\#todo\n\n````python\nimport matplotlib.pyplot as plt\n\nres = lm_fit.resid\nplt.acorr(res)\n````\n\nQQplot\n\nPylab package #todo\nPlots theoretical quantiles of normal distribution vs sample quantiles\n\nRed line 45 degree line are normal distributed\n\n#### Elastic Net\n\n````python\n\nlm_fit = sm.OLS.from_formula('debtgdp ~ rgdpmad', data = df)\nprint(lm_fit.fit().summary())\n\nlm_fit.fit_regularized(method='elastic_net', alpha=0.5, L1_wt=1.0)\n````\n\n#### Quantile Regression\n\n````python\nquantreg = smf.quantreg(\"export ~ rdgpppc\", data = df)\nres = quantreg.fit(q=0.5)\nprint(res.summary())\n````\n\n## Sklearn module\n\n````python\nfrom sklearn.linear_model import LinearRegression\nY = d1.debtgdp.values.reshape(-1,1)\nX = d1.\n````\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/QQ-Plot":{"title":"QQ Plot","content":"\nCreate a theoretical normal distribution\n\nPlots the quantiles of that theoretical normal distribution\n\nMapped hte quantile sof the resideuals against the theoretical noral dsitrubtion\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Qnorm":{"title":"Qnorm","content":"\nQ standas for qunailte\n\nUsed for Inverse CDF\nThe qnorm function in R is used to calculate the inverse of the standard normal cumulative distribution function. In other words, it determines the value of x for a given probability p, where x is a random variable following a standard normal distribution.\n\nThe syntax for using qnorm in R is as follows:\n\nqnorm(p, mean = 0, sd = 1, lower.tail = TRUE)\n\nwhere p is the probability value for which we want to find the corresponding x value. The mean and standard deviation parameters are optional and are set to 0 and 1 by default (since we are working with a standard normal distribution). Finally, the lower.tail parameter specifies whether to calculate the probability from the left or right tail of the distribution.\n\nFor example, suppose we want to find the value of x for which there is a 90% probability that a randomly selected observation falls below it. We can use the following code:\n\nqnorm(0.9)\n\\[1] 1.281552\n\nThis tells us that x = 1.281552 corresponds to a probability of 0.9 in a standard normal distribution.\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Quantile-Regression":{"title":"Quantile Regression","content":"\nEstimation method that is in the same group\n\nWhere in the x variable you are\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/R":{"title":"R","content":"\n[Data Manipulation in R](Data%20Manipulation%20in%20R.md)\n\n[Regular Expression in R](Regular%20Expression%20in%20R.md)\n\n[R Markdown](R%20Markdown.md)\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/R-Markdown":{"title":"R Markdown","content":"\nWrap long lines in PDF output\n\nInstall: 'formatR'\n\n````\n```{r, echo = F}\nlibrary(knitr)\nopts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)\n```\n````\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/R-Packages":{"title":"R Packages","content":"\n[R](R.md)\n\n## DescTools\n\nA collection of miscellaneous basic statistic functions and convenience wrappers for efficiently describing data.\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Random-Experiment":{"title":"Random Experiment","content":"\nEDIT!\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Random-Sampling":{"title":"Random Sampling","content":"\nIt is important to consider excluded group in order to normalize according to your sample.\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Random-Variable":{"title":"Random Variable","content":"\nA random variable is like a box with numbers in it. Every time you reach in and pull out a number, it's a surprise—you never know what number you're going to get! Sometimes the numbers in the box are big, sometimes they're small. But whatever number you get, it's always a random number.\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Randomized-Control-Trial-RCT":{"title":"Randomized Control Trial (RCT)","content":"\n\"Gold standard\" for [Causal Inference](Causal%20Inference.md)\n\nVery rare in economics outside of laboratory settings\n\n### Medical Trials:\n\nPhase 1: rats\nPhases 2: RCT on a few hundred hu8mans\nPhase 3: RCT on thousnads of people\n\na = {a1,a2}\n\na1 = stop investing this thing\na2 = go to phase 3\n\nFalse Discovery: we choose a2\nFalse Omission: we choose a1\n\nWith a false omission we miss out on potentially discovering a blockbuster, and it might be more expensive too\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Redistribution":{"title":"Redistribution","content":"\nMarket outcome may be inequitable\n\nGains from an efficient market could accrue to a disproportionate share of high-income individuals\nTax and transfer to redistribute\n\nIn practice, taxes distort behavior, lead to efficiency loss\n\nOkun's leaky bucket: lose some surplus when transferring form one person to another\n\n## Pareto Optimality\n\nHow to choose between outcomes 1 and 2?\n\nPareto Optimality: Outcome is pareto efficient of you can't amke one person better off without making another worse off\n\nIf all individual sprefer one la #todo **Can be found in Lecture 2 slides - Econ 201**\n\n## Summers/Pritchett memo\n\nMemo while Chief Economist at the World Bank\n\n\" \" #todo \n\n## Social Welfare function\n\nFunction $\\Large W(U_a, U_b)$, so that W is more desireable from society's point of view\n\n\\#todo *check lecture recording at 1:02:00*\n\n**Utilitarian welfare function:**\n\n$\\Large W(U_A, U_B) = \\theta_A U_A + \\theta_B U_B$\n\n* $\\theta$'s are weights, can differ across individuals, be proportional to income, etc.\n\n**Egalitarian:**\n\n$\\Large W(U_A, U_B) = U_A + U_B - \\lambda \\sum_i  \\[U_i - min{U_A,U_B}]$\n\n* Care about inequality specifically in addition to total utility\n\n**Rawlsian:**\n\n$\\Large W(U_A, U_B) = min{U_A,U_B}$\n\n#### Criticisms of utilitarian SWF\n\nWhose utility included? Future generations\n\nPaternalism?\n\nSocial preferences, peer effects, and keeping up with the Joneses\n\nInequality directly affects individual happiness\n\nBase public policy only on individual preferences? What about ethics? Who decides ethics?\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Regression":{"title":"Regression","content":"\n[Linear Regression Model](Linear%20Regression%20Model.md)\n\nVery important concept in economics\n\nlog(y2) - log(y1)\n\nlog(y2/y1) approximately equal to the %difference\n\nAs long as you are dealing with small percentage\n\n## Robustness\n\n[Robust Regression](Robust%20Regression.md)\n\n## Interactions\n\n[Interaction Terms](Interaction%20Terms.md)\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Regression-Discontinuity":{"title":"Regression Discontinuity","content":"\n![Screenshot 2023-01-31 at 6.36.48 PM.png](Image%20Bank/Screenshot%202023-01-31%20at%206.36.48%20PM.png)\n\nInterested in the discontinuity of the outcome variable\n\nCan be used with cross-sectional data\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Regular-Expression":{"title":"Regular Expression","content":"\nThe square brackets `[ ]` in regular expressions are used to define a character class, which means any character inside the brackets is a valid match for that position in the pattern. In this case, the character class `[+-]` specifies that the regular expression should match any string that contains either a plus sign or a minus sign immediately following the characters \"BBB\".\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Regular-Expression-in-R":{"title":"Regular Expression in R","content":"\nIn R, `grepl()` is a function that allows you to search for a pattern within a character vector, and it returns a logical vector indicating whether a match is found or not. The function syntax is as follows:\n\nphpCopy code\n\n`grepl(pattern, x, ignore.case = FALSE, perl = FALSE, fixed = FALSE, useBytes = FALSE)`\n\nHere is a brief explanation of each of the arguments in this function:\n\n* `pattern`: The regular expression pattern to search for.\n* `x`: The character vector to search within.\n* `ignore.case`: A logical value indicating whether the search should be case-insensitive or not.\n* `perl`: A logical value indicating whether the `pattern` argument should be treated as a Perl-compatible regular expression (TRUE) or a basic regular expression (FALSE).\n* `fixed`: A logical value indicating whether the `pattern` argument should be treated as a fixed string (TRUE) or a regular expression (FALSE).\n* `useBytes`: A logical value indicating whether the `pattern` argument should be interpreted as a sequence of bytes rather than a sequence of characters.\n\nHere's a basic example of how to use `grepl()` to search for a pattern in a character vector:\n\n`# create a character vector my_vector \u003c- c(\"apple\", \"banana\", \"orange\", \"pear\")  # use grepl to search for \"apple\" in the vector grepl(\"apple\", my_vector)`\n\nIn this example, the `grepl()` function searches for the regular expression pattern \"apple\" within the `my_vector` character vector. The function returns a logical vector `[TRUE FALSE FALSE FALSE]`, indicating that the first element of the vector matches the pattern while the other elements do not.\n\nYou can also use regular expressions to search for more complex patterns. For example, the following code searches for any elements in `my_vector` that contain the letters \"a\" and \"e\" in any order:\n\n`grepl(\"a.*e|e.*a\", my_vector)`\n\nThis code uses the regular expression pattern `a.*e|e.*a` to match any string that contains the letters \"a\" and \"e\" in any order. The function returns a logical vector `[TRUE FALSE TRUE FALSE]`, indicating that the first and third elements of the vector match the pattern while the other elements do not.\n\nn regular expressions, the pattern `X[0-9]{4}$` matches any string that starts with the letter \"X\", followed by four digits, and ends with the end of the string (`$` represents the end of the string).\n\nMore specifically:\n\n* `X` matches the letter \"X\" literally.\n* `[0-9]` matches any digit from 0 to 9.\n* `{4}` matches exactly four occurrences of the previous expression, in this case `[0-9]`.\n* `$` matches the end of the string.\n\nSo, in the context of selecting column names in a data frame, `X[0-9]{4}$` would match any column name that starts with \"X\" and is followed by exactly four digits, with no other characters after the digits. For example, it would match column names like \"X2014\", \"X2015\", \"X2016\", but not \"X20171\" or \"Y2015\".\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Regularization":{"title":"Regularization","content":"\n## Model Complexity\n\nRunning a [Regression](Regression.md) with many regressors or powers can increase the complexity of the model\n\n**Least complex model:**\n\n* Null Model: uses no regressors\n\nHaving some complexity is useful for prediction and model inference (we do not live in a simple world where x just affects y and that's it)\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Resampling":{"title":"Resampling","content":"\nAllows for creation of useful models that can more confidently answer questions about the data\n\nI believe it is more useful with smaller than ideal datasets\n\nSmaller data = less confidence in effect sizes and significance levels\n\nPredictions based on the whole dataset can only reflect what the model already knows, meaning that if our model is biased, our assumptions violated or we overfit, then our predictions might just be wrong\n\n## Methods\n\n[Training-test split](Training-test%20split.md)\n\n[Cross-Validation](Cross-Validation.md)\n\n[Bootstrap](Bootstrap.md)\n\n## Why do we use resampling\n\nGet a better fit on the data\n\n![Screenshot 2023-05-07 at 4.16.41 PM 1.png](Image%20Bank/Screenshot%202023-05-07%20at%204.16.41%20PM%201.png)\n\nIn Machine learning we mainly care about overfitting:\n\nGreat predictions on the training data\n\nBad predictions on the test data\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Rescaling":{"title":"Rescaling","content":"\nAccount for jumps and lows\n\nvector - min((vector)) / (max(vector) - min(vector))\n\nStandardize = subtract mean and divide by standard deviation\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Residual":{"title":"Residual","content":"\nDifference between observed and predicted value of each observation\n\nI believe in terms of linear regression it is synonymous with the error\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Reverse-Causality":{"title":"Reverse Causality","content":"\n\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Robust-Regression":{"title":"Robust Regression","content":"\nProvides a solution to [Outliers](Outliers.md) and high leverage observations\n\n* Gives different robustness weights from 0-1 to every observations based on its [Residual](Residual.md)\n* Assignment of weights (at least in the 'robustbase' R package) happens through [Iteratively Reweighted Least Squares](Iteratively%20Reweighted%20Least%20Squares.md)\n\n![Screenshot 2023-05-07 at 7.01.40 PM.png](Image%20Bank/Screenshot%202023-05-07%20at%207.01.40%20PM.png)\n\n## What is the difference between Robust regression and Robust Standard Error\n\nThe first one is to control outliers and the [Robust Standard Errors](Robust%20Standard%20Errors.md) is for heteroscedasticity\n\n## Why don't we remove outliers and run OLS\n\nIn most cases it is a bad idea:\n\n* We would **lose information/observations**\n* Robust regression **lowers the weights** of outliers so it still provides some useful information to our model without dominating or having a large influence\n\n## Coding\n\n````r\n# conduct robust regression\nlibrary(robustbase)\nrm \u003c- lmrob(outcome ~ predictor, data = df)\n\nsummary(rm)\n````\n\n#### Plotting the residuals\n\n````r\n# plot residuals from our original non-robust model for demonstration purposes to show the weighting in a robust regression\nlibrary(sjPlot)\nplot_residuals(m)\n````\n\nObservation 1 and 3 have the smallest residual and therefore the largest weight\n\nTherefore observation 4 in this case would have a very small if not zero weight thus having no influence on the model\n\n![residuals plot.png](Image%20Bank/residuals%20plot.png)\n\n## Comparing Robust Regression and OLS\n\n````r\n# visualizing both models\nplot_model(m, type = \"pred\", show.data = T)\nplot_model(rm type = \"pred\", show.data = T)\n\n\n# compare coefficients and R^2 of both models\ntab_model(m)\ntab_model(rm)\n````\n\n![Screenshot 2023-05-07 at 7.54.29 PM.png](Image%20Bank/Screenshot%202023-05-07%20at%207.54.29%20PM.png)\n\nHere we could look at the adjusted R^2 to see which fits the data better\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Robust-Standard-Errors":{"title":"Robust Standard Errors","content":"\nRobust”standard errors is a technique to obtain unbiased standard errors of OLS coefficients under [heteroscedasticity](https://economictheoryblog.com/2016/02/06/clrm-assumption-4/).  Remember, the presence of heteroscedasticity violates the [Gauss Markov assumptions](https://economictheoryblog.com/2015/02/26/markov_theorem/) that are necessary to render [OLS](https://economictheoryblog.com/ordinary-least-squares-ols/) the best linear unbiased estimator (BLUE).\n\n\\#todo \n\nhttps://data.library.virginia.edu/understanding-robust-standard-errors/\n\nThis resource above has some great R and Stata code and the matrix math behind it too\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Rolling-Window-Regressions":{"title":"Rolling Window Regressions","content":"\nHow the movement between two variables chhanged across 100 day periods?\n\nDaily  observations = 60 days of data\n\nless observation = degrees of freed om is too low\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/STAT-206-Feb-6-Office-Hours":{"title":"STAT 206 Feb 6 Office Hours","content":"\nYoure aabout to observe a data strem (y1, y2, ... ); all of these Yi values are unknown to you before dataset arrives (this is a prediction problem)\n\nBayesian and Frequentist would try to predict y (Line on top)\\_i\n\nY(line on top)\\_i = OBserved value of observation number i uin sequence\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/STAT-206-THT-1":{"title":"STAT 206 THT 1","content":"\nPoisson is for discrete data (quiz 3)\n\n---\n\n(2B)\n\nExponential:\n\np(yi|\\[SME])\n\nSomething it will be parameterize in terms of lambda or sometimes in terms of 1/lambda\n\nAssume:\n\n$y_i \u003e 0$ \n\n2B(1)\n\nNormal quantile-quantile plot: sampling model criticism\nAlso known as [QQ Plot](QQ%20Plot.md)\n\nPlot the CDF of N(same mean \u0026 SD as data) and plot the empirical CDF on top\n\nEmpirical CDF = f(hat)*n(t|y) = (number of yi \\\u003c or equal t)/n*\n\nExchangeability is \n\nnon-negative integers\n\nImagine if we tried to model this dataset discretely?\n\nHow many support points would the discrete distribution at an absolute minimum?\n\nWhat does it mean to model things discretely?\n\n[Support Points](Support%20Points.md) as possible values before you saw them\n\nY_i as a discrete random variable = the support set would have to \n$$y_i \\epsilon \\space {0,1,2,...,21195}$$ \nwithin brackets are all support points but perhaps 0 is not included\n\nWe have to set 21194 probability mass functions?\n\nThus we can say we have too many support points, it would be much better to model the ...?? $$y\\_{i} \\space \\epsilon \\space (0,\\infty)$$\n\n---\n\n![Screenshot 2023-02-21 at 5.46.47 PM.png](Image%20Bank/Screenshot%202023-02-21%20at%205.46.47%20PM.png)\n\nMUch better to model as continous underlying process:\n\nWhen running Professor's Draper's code for 2B\n\nPart of R code is not applicable\n\n**Posterior distribution looks like :**\n\n![Screenshot 2023-02-19 at 3.44.20 PM.png](Image%20Bank/Screenshot%202023-02-19%20at%203.44.20%20PM.png)\n\nNormal distribuition\n\nalpha/2 left \n\np(lamba|exponential sampling model)\n\nShows you can construct a Bayesian interval:\n\n* We pretend this distirbution this is a normal curve\n\noh god haha that sounds gnarly\ni guess 11th hour it is\n\ni like the vibes at the abbbey tho so maybe another time\n\n---\n\n2B2\n![Screenshot 2023-02-19 at 3.52.02 PM.png](Image%20Bank/Screenshot%202023-02-19%20at%203.52.02%20PM.png)\nDemonstrating that a conjugant prior exists:\n\n1. Write down the likelihood function, consider it as a member of the fmaily of probability density functions: it is a inverse gamma as think of its as an unnormalized fgunction of lambda\n\n1. Look that is a member of the inverse gamma family: look in Gelman's appendix: look if it is a reognized member of \n\n1. The product of two inverse gamma's is another one: inverse gamma\n\nWe do have a conjugant prior fmaily\n\nGo ahead and multiply everything out \n\n---\n\nInformation in MLE equals:\n\n$$ \\Large \\hat I (\\hat\\lambda\\_{MLE}) $$\n\n* Which is order N and goes up linearly\n* Represents how much we know about lambda\n\n[Variance](Variance.md) represents how much we do not know about lambda\n\nVariance is inversely related to the information\n\nEstimated Frequentist sampling variance of the MLE = \n\n![Screenshot 2023-04-01 at 5.23.06 PM.png](Image%20Bank/Screenshot%202023-04-01%20at%205.23.06%20PM.png)\n![Screenshot 2023-04-01 at 5.20.48 PM.png](Image%20Bank/Screenshot%202023-04-01%20at%205.20.48%20PM.png)\n\n![Screenshot 2023-04-01 at 5.37.12 PM.png](Image%20Bank/Screenshot%202023-04-01%20at%205.37.12%20PM.png)\n\n100(1-alpha)% large N approximate Confidence Interval for lambda = \n\nlambda.hat.mle +/- Z number from the standard normal curve also denoted as: \n\n$$\\Large \\Phi^{-1}(1-\\frac{\\alpha}{2})$$\n\nmultiplied by the estimated standard error.\n\nThe inverse Phi is also known as the [Qnorm](Qnorm.md) funcion in [R](R.md)\n\n## Inverse Gamma Distribution\n\n|Us|R|\n|--|-|\n|$\\Large \\alpha$|shape|\n|$\\Large\\beta$|rate|\n\ninvgamma package function in R\n\n![Screenshot 2023-04-03 at 3.33.36 PM.png](Image%20Bank/Screenshot%202023-04-03%20at%203.33.36%20PM.png)\n\n[Bernstein-von Mises Theorem](Bernstein-von%20Mises%20Theorem.md) says that under regularity conditions for large n, that likelihood inference is roughly equal to Bayes inference with a low information prior.\n\n---\n\nBayes is great:\n\n* Always produces results that are logically internally consistent\n  * Frequentist methods (both Neyman style and Fisher style) can violate [Logical Internal Consistency](Logical%20Internal%20Consistency.md)\n\nBayes can be terrible:\n\n* nothing in the Bayesian inferential paradigm guarantees good calibration with the real world\n* calibration: inferential conclusions that get the right answer about as often as we've said they would\n\nExample: How to break Bayes\n\n* Use high information prior that is completely out of step with reality.\n\nThe Bayesian approach focuses on cohererence and logical consistency, while the frequesntist approach focuses more on calibration\n\nThese two focuses together help to make our uncertainty assessments to be both internally and externally consistent\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/STAT-206-THT-2":{"title":"STAT 206 THT 2","content":"\nFisher Scoring (MLE) is Newton-Raphson in disguise\n\n* In the basic diagram that illustrates the frequentist inferential paradigm — with the pop- ulation, sample and repeated-sampling data sets, each containing N, n, and M elements, respectively (see, e.g., page 6 of the LDS document camera notes from 31 Jan 2023), and with the sample drawn from the population in an IID manner — when the population parameter of main interest is the mean θ and the estimator is the sample mean Y ̄, as long as the population SD σ satisfies (0 \\\u003c σ \\\u003c ∞) You will always get a Gaussian long-run distribution for Y ̄ (in the repeated-sampling data set) as long as any one of (N,n,M) goes to infinity.\n  * Answer: only if n goes to infinity, the other two are irrelevant to CLT.\n\nStatistical Inference: Induction\n\nProbability: deduction\n\n![Pasted image 20230325164547.png](Image%20Bank/Pasted%20image%2020230325164547.png)\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Sampling-Model-Uncertainty":{"title":"Sampling Model Uncertainty","content":"\nWe cannot infer the sampling model from the context\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Scale-Location-Plot":{"title":"Scale Location Plot","content":"\nCook's distance\n\nSquare root of standardized reisudal sversus fitted values of y for checking i.i.d assumption\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Selection-Bias":{"title":"Selection Bias","content":"\nWe may only have observations that do get tested or receive treatment\n\nSuppose two groups have the same distribution:\n\n![Screenshot 2023-01-31 at 5.56.52 PM.png](Image%20Bank/Screenshot%202023-01-31%20at%205.56.52%20PM.png)\n\nYou don't observe some of the data\n\nYou dont know the distribution of the data you do not observe:\n\n![Screenshot 2023-01-31 at 5.59.07 PM.png](Image%20Bank/Screenshot%202023-01-31%20at%205.59.07%20PM.png)\n\nOne dimension that you care about and you don't observe it for some subsample\n\nIf they have different distributions then you will make errors with [Causal Inference](Causal%20Inference.md)\n\nCovariant Distribution of X and Y determines selection into data and this is problematic isf this is also the relaitonship we look at for our outomce\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Social-Safety-Net-Spending":{"title":"Social Safety Net Spending","content":"\nPrograms can be categorical or means-tested\n\n**Categorical welfare:** Eligibility determined by some demographic e.g. single motherhood or disability\n\n**Means-tested welfare:** Eligibility determined by income and asset levels\n\nThey can also be cash or in-kind\n\n**Cash welfare:** Provide cash benefits to recipients\n\n**In-kind welfare:** Deliver goods, such as medical care or housing, to recipients\n\n## Cash-based means-tested\n\nBenefit amount = benefit guarantee - BRR * earned income\n\n* BRR = 50% in California (Benefit Reduction Rate)\n  * Phases the benefits out\n  * Every dollar you earn gets taxed away at 50%\n  * 0% would be universal basic income\n\n## Poverty Measurement\n\n1. Absolute: Disposable income \\\u003c Threshold value\n1. Relative: Disposable income \\\u003c threshold value determined by median (e.g. less than 30% of median)\n\nAbsolute poverty falls in the long run with economic growth \\[nobody in the US is World Bank poor] but [relative poverty](relative%20poverty.md) does not\n\nAbsolute poverty captures both growth and inequality effects while relative poverty caputres only inequality effects\n\n## Economic Framework\n\n[Welfare and Labor Supply](Welfare%20and%20Labor%20Supply.md)\n\nEITC and labor supply\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Solow-Model":{"title":"Solow Model","content":"\n## Production Function\n\n#### Assumptions\n\n* Constant returns to scale\n\n#### Intensive Form\n\nAssumed to satisfy:\n\n* Zero input = zero output\n* As you increase k, production should grow\n* As f(k) increases, it increases at a slower rate\n\nInada conditions\n\n## Investment and Consumption\n\nWe assume there is no government and the economy is a closed economy\n\nG = 0\nNX = 0\n\nY = C + I\n\nDivide both sides by L\n\nper unit of Labor\n\ny = c + i\n\nAssume savings equals the economy's investment and denote s as te saving rate (or investment rate). Then we have:\n\ni = sy\ni = sf(k)\nc = (1-s)y\n\n## Capital Growth\n\n## Full Law of Motion\n\n# Capital Account Lib. is an extension of Solow Model but Neoclassical is a different model\n\n[Capital Account Liberalization and the Neoclassical Growth model](Capital%20Account%20Liberalization%20and%20the%20Neoclassical%20Growth%20model.md)\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Spatial-Mobility":{"title":"Spatial Mobility","content":"\n## Agglomeration and superstar cities\n\nAgglomeration refers to the concentration of economic activity and population in a particular region or area, often leading to the creation of urban areas or cities. It involves the clustering of industries, firms, and people in a specific location to take advantage of economies of scale, shared resources, and other benefits associated with proximity. Agglomeration can create positive externalities such as knowledge spillovers, innovation, and greater market access. It can also lead to negative externalities such as congestion and pollution.\n\nThis has gotten stronger due to the knowledge economy\n\n## [Intergenerational Mobility](Intergenerational%20Mobility.md) by area\n\n[Gini Coefficient](Gini%20Coefficient.md)\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Standard-Deviation":{"title":"Standard Deviation","content":"\nThis is the square root of the [Variance](Variance.md)\n\n![Standard-Deviation-ADD-SOURCE-e838b9dcfb89406e836ccad58278f4cd.jpg](Image%20Bank/Standard-Deviation-ADD-SOURCE-e838b9dcfb89406e836ccad58278f4cd.jpg)\n\n![variance-and-standard-deviation-4-1651274333.jpg.png](Image%20Bank/variance-and-standard-deviation-4-1651274333.jpg.png)\n\n![Variance-and-Standard-Deviation.png](Image%20Bank/Variance-and-Standard-Deviation.png)\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Standard-Errors":{"title":"Standard Errors","content":"\nAlso known as sampling uncertainty\n\nTaing the imperical standard deviation\n\nSample variance or sample standard deviation\n\nTaking the square roott of ht esample variance of \n\nApplying Basel's correction:\n\n$$ \\sqrt{\\frac{1}{B-1}\\sum^{N}\\_{i=1}()} $$\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Standard-Normal-Distribution":{"title":"Standard Normal Distribution","content":"\nAlso known as: *Bell Curve* or a *Gaussian Distribution*\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Standardize":{"title":"Standardize","content":"\n![2018-10-image5-9-1024x591.jpg](Image%20Bank/2018-10-image5-9-1024x591.jpg)\n\nZ score shows you where the value X of a particular non-standard normal distribution would lay on the [Standard Normal Distribution](Standard%20Normal%20Distribution.md)\n\ni.e. you would get the same area above/below the value X (which is the probability)\n\nP(Z \u003e 1.5) = 0.067 as an example\nThus: \nP(Height \u003e 190) = 0.067\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Stata":{"title":"Stata","content":"\n[t-test](t-test.md)\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Statistical-Inference":{"title":"Statistical Inference","content":"\nTwo main approaches:\n\n* [Confidence Interval](Confidence%20Interval.md)\n* [Hypothesis Testing](Hypothesis%20Testing.md)\n* \n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Statistical-Learning":{"title":"Statistical Learning","content":"\nTasks it can help with:\n\n* Prediction\n* Identifying images\n* \n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Statistics":{"title":"Statistics","content":"\nIt statistics we are comparing things\n\nTypes:\n[Bayesian Statistics](Bayesian%20Statistics.md)\n\n[Frequentist Statistics](Frequentist%20Statistics.md)\n\n## Important Concepts\n\n#### Variables\n\nData that can vary (thus not a constant)\n\n[P-Value](P-Value.md)\n\n## Methods\n\n[Random Sampling](Random%20Sampling.md)\n\n[Hypothesis Testing](Hypothesis%20Testing.md)\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Stocashtic-volTILITY":{"title":"Stocashtic volTILITY","content":"\nARCHGAM\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Support-Points":{"title":"Support Points","content":"\n$$y\\_{i} \\rightarrow y\\_{i} \\space \\epsilon \\space {0,1,2,3,...}$$\n\nlist of potential values in the support set (a.k.a sample space)\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Support-Vector-Machines-SVM":{"title":"Support Vector Machines (SVM)","content":"\nClassification:\n\nConvert outcome variable to a factor\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Synthetic-Control":{"title":"Synthetic Control","content":"\nRequires same assumptions as [Difference-in-differences](Difference-in-differences.md)\n\nData-driven weights from pre-period\n\nOften used when a single cluster is treated\n\n![Screenshot 2023-01-31 at 6.42.36 PM.png](Image%20Bank/Screenshot%202023-01-31%20at%206.42.36%20PM.png)\n\nNot credible counterfactual often since different levels and trends\n\nRidge regression for weighting\n\nPermutation test?\n\nConformal inference? with time-series\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Tail-Event":{"title":"Tail Event","content":"\ne.g. recession\n\ngreat losses\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Tax-Efficiency":{"title":"Tax Efficiency","content":"\n## Introduction\n\nGovernment raises taxes for one of two reasons:\n\n1. To raise revenue to finance public goods\n1. To redistribute income\n\nBut to generate $1 of revenue, welfare of those taxed falls by more than $1 because the tax distorts #todo \n\n## Equilibrium\n\nConsider again the partial equilibrium model\n\n\\#todo \n\n#### Excess burden\n\n![IMG_6204 copy.jpg](Image%20Bank/IMG_6204%20copy.jpg)\n\nDWL = 1/2($\\Delta$Q)t\n\n![IMG_6205 copy.jpg](Image%20Bank/IMG_6205%20copy.jpg)\n\n![IMG_6207 copy.jpg](Image%20Bank/IMG_6207%20copy.jpg)\n\nExcess Burden rises with Tax Squared #todo \n\n## Sufficient Statistics\n\n2 goods $\\large x = (x_1,x_2)$; prices $\\large (p_1,p_2)$; income Z\n\nUtility: $\\large U(x_1,x_2)$ = $\\large ln(x_1) + x_2$ -\u003e **Quasilinear**\n\nNormalize #todo \n\n---\n\nSubstitute budget constraint into objective function:\n\n$$\\Large max \\space U = ln(x_1) + Z - p_1+t)x_1$$\n\nTo solve problem:\n\n* Find $\\Large x_1^\\*$ satisfying $\\Large dU/dx_1 = 0$ \n\nDefine social welfare as sum of individual's\n\n\\#todo research envelope theorem\n\n\\#todo finish\n\n## Sufficient Statistics Approach to Welfare\n\nWhat is a sufficient statistic for the welfare effects of taxation?\n\nSocial welfare function:\n\n$$\\Large W(t) =  {} $$\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Tax-Incidence":{"title":"Tax Incidence","content":"\nThe study of the effects of tax policies on prices and the distribution of utilities\n\nWhat happens to market prices when a tax is introduced or changed?\n\n* Increase tax on cigarettes by $1 per pack\n* EITC\n  * make effective wage higher\n* Food stamps programs\n\nEffect on price -\u003e distributional effects on smokers, profits of producers, shareholders, farmers, ...\n\nEconomic vs. statutory (or legal) incidence:\n\n* Equivalent when prices are constant but not in general\n* Statutory - who legally should pay\n\n## Overview\n\n* Ideally, we would characterize the effect of a tax change on utility levels of all agents in the economy\n* Useful simplification: lump economics agents into broad groups\n* Incidence analyzed at a number of levels:\n  * Producer vs. consumer (tax on cigarettes)\n  * Source of income (labor vs. capital)\n  * Income level (rich vs. poor)\n  * Region or country (local property taxes)\n  * Across generations (social security reform)\n\n## Partial equilibrium incidence:\n\n#### Model setup\n\nFirms' cost of producing of C(S), where S is the quantity supplied c'(S) \u003e 0 and c''(S) $\\ge$ 0\n\n* The first derivative means it is an **increasing function** thus cost rises as quantity supplied increases (Marginal Cost)\n* The second derivative means that there the shape of the curve is convex (I believe it means decreasing returns to scale)\n\nProfit at a pretax price p and level of supply S is pS - c(S)\n$$\\large S = pS - c(S)$$\n\n* pS is the revenue (price \\* Supply/Quantity)\n* c(S) is the cost\n\nWith perfect optimization, the supply function for good x is implicitly defined by the marginal condition p = c'(S(p)). Does this relate to Marginal revenue equals marginal cost and can be say p = marginal revenue here?\n\n*The marginal condition p = c'(S(p)) is equivalent to marginal revenue equals marginal cost in perfect competition. In this case, the price (p) that a firm receives for its product is equal to the additional cost (c') of producing an extra unit of output at the level of supply (S(p)). Therefore, we can interpret p as the marginal revenue earned by the firm.* #todo\n\n*The relationship between the marginal condition p = c'(S(p)) and the principle \"marginal revenue equals marginal cost\" is that both are used to determine the optimal level of production for a firm.* \n\nThe marginal condition p = c'(S(p)) represents the point at which the price of a good equals the cost of producing an additional unit of that good. It is essentially a supply-side perspective on determining optimal production levels.\n\nOn the other hand, \"marginal revenue equals marginal cost\" represents the point at which the additional revenue generated by producing one more unit of a good is equal to the additional cost incurred in producing that unit. This principle is often used from a demand-side perspective, as it helps determine how much consumers are willing to pay for each additional unit of a good.\n\nHowever, both approaches ultimately aim to find the level of production that maximizes profit for a firm. By setting marginal revenue equal to marginal cost or finding where price equals cost, firms can optimize their production levels and avoid inefficiencies such as overproduction or underproduction.\n\nIn addition, tax incidence can also be analyzed using these principles. When a tax is imposed on a good, it affects both supply and demand curves and ultimately changes the equilibrium price and quantity. By analyzing how taxes affect supply and demand curves, economists can determine who bears most of the burden of taxation - either producers or consumers - and how much each side bears relative to each other.\n\nThe equation p = c'(S(p)) represents the condition where the price (p) equals the marginal cost of production (c') at a given level of supply (S(p)). This is similar to the condition in MR=MC, where the marginal revenue earned from selling an additional unit of a good is equal to its marginal cost of production.\n\nBoth conditions are important in determining the equilibrium price and quantity of a good in a market. In a perfectly competitive market, where there are many buyers and sellers and no barriers to entry or exit, the supply function is perfectly elastic and the market price is determined by the intersection of the supply and demand curves. At this equilibrium point, MR=MC holds true for all firms in the market, ensuring that resources are allocated efficiently.\n\nHowever, when taxes are imposed on a good, they affect both consumers and producers by altering their behavior. The incidence of tax can be analyzed using either the demand or supply curve. If we analyze it using the supply curve, we can see that taxes shift it upward by an amount equal to the tax per unit of output. This results in an increase in price paid by consumers and a decrease in net revenue received by producers. The extent to which each group bears this burden depends on their relative price elasticities of demand and supply. \n\nIn summary, understanding tax incidence requires an understanding of both supply and demand functions as well as their relationship with MR=MC.\n\nThe marginal condition p = c'(S(p)) means that the price (p) of good x is equal to the marginal cost (c') of producing it at the level of supply (S(p)). In other words, producers will only supply good x at a price that covers their marginal costs. \n\nThis concept is important in understanding tax incidence, which refers to how the burden of a tax is distributed between buyers and sellers in a market. When a tax is imposed on good x, it increases the cost of production for sellers and therefore shifts the supply curve upwards. As a result, the equilibrium price for good x increases, but the quantity demanded decreases. \n\nThe incidence of the tax depends on the relative elasticities of demand and supply. If demand is relatively inelastic (i.e., buyers are not very responsive to changes in price), then most of the burden of the tax will fall on buyers as they continue to purchase relatively large quantities at higher prices. If supply is relatively inelastic (i.e., sellers are not very responsive to changes in price), then most of the burden will fall on sellers as they continue to produce relatively large quantities despite facing higher costs.\n\nIn summary, understanding the relationship between price, marginal cost, and supply is key to understanding how taxes affect market outcomes and who bears their burden.\n\n**Perfect optimization** in this context refers to the condition where suppliers of good x are able to produce and sell the good at the **lowest possible cost, while also maximizing their profits.** This involves making efficient use of resources and minimizing waste, while also ensuring that the price of the good is set at a level that is both competitive and profitable. By achieving perfect optimization, suppliers are able to operate at maximum efficiency and generate the highest possible returns on their investment. The supply function for good x is implicitly defined by this marginal condition because it represents the point at which suppliers are able to balance their costs with the price they charge for the product, resulting in a state of equilibrium where supply meets demand.\n\nLet denote $\\Large \\epsilon_S$ denote the price elasticity of supply\n\n#### Equilibrium\n\nGovernment levies an excise tax on good x\n\nThis is a tax that is levied on a quantity (e.g. gallon, pack, ton)\n\n**The elasticities of demand and supply** determine **tax incidence** in this case\n\nAn excise tax is a tax on specific goods or services, such as cigarettes or gasoline. The tax is usually added to the price of the product and paid by the consumer at the point of sale. \n\nHowever, the incidence, or burden, of the tax may be shared between producers and consumers depending on the elasticity of supply and demand. \n\nGenerally, if demand is more elastic than supply, producers will bear a larger portion of the tax burden, while if supply is more elastic than demand, consumers will bear a larger portion.\n\nAd-valorem tax: fraction of prices (e.g. sales tax)\n\nNow with the tax:\n\nq = p + t\n\n* q = post-tax price (tax inclusive price)\n* p = pre-tax price\n* t = tax\n\nEquilibrium:\n\n$$\\large Q = S(p) = D(p+t)$$\nThe equilibrium with an excise tax of t is given by the intersection of the supply curve S(p) and the demand curve D(p+t). This is because the excise tax increases the cost of production for suppliers, shifting the supply curve upwards by the amount of the tax.\n\nThe tax burden falls on both suppliers and consumers, but it is determined by their respective price elasticities of demand and supply. If demand is relatively inelastic (less responsive to price changes), then consumers will bear most of the tax burden. Conversely, if supply is relatively inelastic (less responsive to price changes), then suppliers will bear most of the tax burden.\n\nTo determine who bears the tax burden, we can use the concept of price elasticity. The price elasticity of demand measures how much consumers' quantity demanded changes in response to a change in price, while the price elasticity of supply measures how much suppliers' quantity supplied changes in response to a change in price.\n\nIf demand is more elastic than supply (i.e., consumers are more sensitive to price changes than suppliers), then consumers will bear a larger share of the tax burden. Conversely, if supply is more elastic than demand (i.e., suppliers are more sensitive to price changes than consumers), then suppliers will bear a larger share of the tax burden. The actual incidence of the tax depends on how elastic each side's response is and varies from case to case.\n\nIn microeconomics, the equilibrium with an excise tax S(p) = D(p+t) is considered because an excise tax on a good or service shifts the supply curve upward by the amount of the tax. This means that producers will need to charge a higher price to cover their costs and maintain their profit margins. As a result, the equilibrium quantity demanded will decrease, as consumers will be less willing to pay for the higher price.\n\nOn the other hand, the demand curve remains unchanged since consumers' willingness to buy is not affected by taxes. However, they will end up paying a higher price for the product due to the increased supply cost. Thus, in this scenario, both producers and consumers share the burden of paying for the excise tax.\n\nThe new equilibrium with S(p) = D(p+t) is found where supply intersects with demand curve after shifting upward by t units. Therefore, this equilibrium reflects an adjustment between buyers and sellers in response to an increase in taxes and is considered optimal as it represents a balance between supply and demand at a new market price level.\n\n#### Side of the market taxed\n\n![Pasted image 20230508181524.png](Image%20Bank/Pasted%20image%2020230508181524.png)\n\npay less to the firm #todo\n\nInclusive with the tax you pay more #todo\n\nVery price unresponsive\n\n![IMG_6200 copy.jpg](Image%20Bank/IMG_6200%20copy.jpg)\n\n![IMG_6202 copy.jpg](Image%20Bank/IMG_6202%20copy.jpg)\n\n## Principles\n\n1. Incidence is shared between consumers and producers\n1. Incidence is neutral with respect to the side of the market taxed\n1. Impact on CS and PS summarized by pass-through rate\n1. Pass-through rate determined by the relative elasticity of supply and demand\n\nExplain the points above in terms of microeconomics\n\n1. Tax incidence refers to the distribution of the burden of a tax between buyers and sellers in a market. In microeconomics, tax incidence is determined by the relative price elasticities of supply and demand. If demand is more elastic than supply, buyers will bear a larger share of the tax burden, whereas if supply is more elastic than demand, sellers will bear a larger share.\n\n1. The neutrality of tax incidence with respect to the side of the market taxed means that regardless of whether the tax is levied on buyers or sellers, the overall impact on the market will be the same. This neutrality is based on the assumption that both buyers and sellers are rational and respond to changes in prices in predictable ways.\n\n1. The impact of a tax on consumer surplus (CS) and producer surplus (PS) can be summarized by a pass-through rate, which measures how much of the tax burden falls on each side of the market. The pass-through rate reflects the relative price elasticities of supply and demand: if demand is more elastic than supply, buyers will bear a smaller share of the tax burden and PS will decline by more than CS; conversely, if supply is more elastic than demand, sellers will bear a smaller share and CS will decline by more than PS.\n\n1. The pass-through rate is determined by the relative elasticity of supply and demand: if one side has a relatively higher elasticity than the other, it will be able to shift more of the tax burden onto its counterpart. For example, if demand for a good is relatively insensitive to changes in price (i.e., inelastic), while supply is relatively responsive (i.e., elastic), then producers will be able to pass on most or all of any new taxes they face to consumers through higher prices. Conversely, if demand is highly elastic while supply is not, then producers will have little room to raise prices without losing sales volume, so they may end up bearing most of the tax burden themselves.\n\n#### Example of a pass-through rate\n\nLet's say a government imposes a tax of $2 per unit on a certain good. The original price of the good is $10 per unit. \n\nIf the pass-through rate is 100%, then the entire burden of the tax will be passed onto consumers in the form of higher prices. The new price of the good would be $12 per unit (original price + tax).\n\nIf the pass-through rate is only 50%, then half of the tax burden will be absorbed by producers and half will be passed onto consumers. In this case, the new price of the good would be $11 per unit (original price + half of tax = $10 + $1 = $11).\n\n## Empirical Applications\n\n[Empirical Research Design](Empirical%20Research%20Design.md)\n\nWhy develop an explicit design rather than simply use all available variation in tax rates in terms of empirical research design in microeconomics?\n\nDeveloping an explicit design for tax incidence research in microeconomics allows for greater control over variables and potential confounding factors. By manipulating and controlling certain aspects of the design, researchers can isolate the specific effects of changes in tax rates on different groups of individuals or industries. This can lead to more accurate and precise estimates of tax incidence, which is particularly important when policymakers are considering changes to tax policy that could have significant impacts on various stakeholders. Additionally, an explicit design can help ensure the validity and reliability of the research findings, thereby increasing their credibility and usefulness to policymakers and other stakeholders.\n\nFor example:\n\n$$\\Large q\\_{it} = \\beta_0 + \\beta_1 T\\_{it} + \\epsilon\\_{it}$$\n\nT = treatment ([Random Experiment](Random%20Experiment.md) random assignment hopefully)\n\n$$\\Large T\\_{it} = \\alpha + \\beta X\\_{it} + \\eta\\_{it}$$\n\nUnobserved eta ... #todo 48:00\n\n\\#todo 45:00\n\nConsider estiamting effect of a treatment (e.g., tax) T on outcome y\n\n$$\\Large y_i = \\alpha + \\beta T_i + \\epsilon_i$$\n\nHow do you control for observables?\n\n* [Ordinary Least Squares](Ordinary%20Least%20Squares.md) regression\n* [Nearest Neighbor Matching](Nearest%20Neighbor%20Matching.md) \n* [Propensity-score reweighting](Propensity-score%20reweighting.md)\n* [LASSO](LASSO.md) if there are many Xu’s, not a lot of observations (high-dimensional data)\n\nA research design is a source of variation in $\\Large \\eta_i$ that is credibly unrelated to $\\Large \\epsilon\\_{i}$\n\nExample: a reform that affects people above age 65 but not below\n\nPeople at age 64 and 65 likely or have similar outcomes\n\n$$\\Large cov(\\eta_i, \\epsilon_i) = 0$$\n\nGeneral lesson: controlling for confounding factors using regression or reweighing will rarely give you convincing estimates\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Taxation":{"title":"Taxation","content":"\n[Tax Incidence](Tax%20Incidence.md)\n\n[Tax Efficiency](Tax%20Efficiency.md)\n\n[Cigarette Tax](Cigarette%20Tax.md)\n\n[Income Taxation](Income%20Taxation.md)\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Tidy-Data":{"title":"Tidy Data","content":"\n![Screenshot 2023-05-07 at 5.18.00 PM.png](Image%20Bank/Screenshot%202023-05-07%20at%205.18.00%20PM.png)\n\n## Why do we want tidy data\n\n* Easy to transform\n* Easy to visualize\n* Easy to manipulate\n* Easy to analyze\n\n![Screenshot 2023-05-07 at 5.19.29 PM.png](Image%20Bank/Screenshot%202023-05-07%20at%205.19.29%20PM.png)\n\nTidying up data before analysis\n\n![Screenshot 2023-05-07 at 5.15.10 PM.png](Image%20Bank/Screenshot%202023-05-07%20at%205.15.10%20PM.png)\n\nWhatever changes in your data - put it into columns\n\n#### Why columns?\n\nEasy to store data that is similar\n\nData cannot be identical\n\nColumns = variables\n\n* which is what we need to make data analysis possible\n\n## Common Problems with data\n\n#### 1. One variable is stored in multiple columns\n\ne.g. when a variable is stored in column names\nThis could be when years are used for columns (see below)\n\nThis is when we would want to make the data **longer**\n\n![Screenshot 2023-05-07 at 5.33.49 PM.png](Image%20Bank/Screenshot%202023-05-07%20at%205.33.49%20PM.png)\n\n#### 2. Multiple variables are stored in one column\n\nObservations in one column that are actually column keys or variables that do not belong together\n\nIn the example below: cases and population do not refer to the same thing and must be separated\n\nNow we would make the data **wider**\n\n![Screenshot 2023-05-07 at 6.39.09 PM.png](Image%20Bank/Screenshot%202023-05-07%20at%206.39.09%20PM.png)\n\n#### 3. More than one value in the same cell\n\nHere we would need to separate values into multiple columns\n\n![Screenshot 2023-05-07 at 6.43.20 PM.png](Image%20Bank/Screenshot%202023-05-07%20at%206.43.20%20PM.png)\n\n#### 4. Different types of data are stored in the same column\n\ne.g. numbers and text are stored in the same volumn\n\n#### 5. Missing values\n\nBe careful: These can sometimes be coded as: \n\n* Text\n  * \"missing\" or \"unknown\"\n* Numeric\n  * 0 or 9999\n\nIn reality it should be left blank\n\n#### 6. Variables are stored in both rows and columns\n\nThe upper part is wrong - the bottom is correct\n\n![Screenshot 2023-05-07 at 6.50.05 PM.png](Image%20Bank/Screenshot%202023-05-07%20at%206.50.05%20PM.png)\n\n#### 7. Other common things to remove/fix:\n\n* No colors\n* No generic headers for multiple columns\n* Unnecessary white space\n* Combine multiple tables into one and communicate difference with a new variable\n* Combine multiple excel sheets into one and communicate difference with a new variable\n* Short and concise column names\n  * Too short and people will not know the meaning\n  * Long column names should be avoided and information should be coded into the variable (no legend or explanations in variable names)\n    * e.g. Male, Female, Other (1,2,3) should be coded as text that can later be converted to categorical/factor during data analysis for R\n    * Other languages it may be fine to keep it as number but then include variable description labels \n\n## Tidying Data is always worth it\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Total-Factor-Productivity":{"title":"Total Factor Productivity","content":"\nDefine total factor productivity in macroeconomics and give an example and the production function\n\nTotal Factor Productivity (TFP) is a measure of the efficiency with which inputs of labor and capital are used in the production process. It is calculated as the ratio of output to the total inputs used in production, including labor, capital, and other factors such as technology and natural resources.\n\nFor example, consider a manufacturing company that produces cars. The company uses machines (capital) and workers (labor) to produce cars. If the company can produce more cars using the same amount of inputs (i.e., machines and workers), then its TFP increases. This can happen through technological advancements, better management practices, or improvements in worker skills.\n\nThe production function for TFP can be expressed as follows:\n\nY = A * F(K,L)\n\nWhere Y is the output produced by using K units of capital and L units of labor, A is the level of total factor productivity, and F represents a function that shows how much output can be produced with a given amount of capital and labor.\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Training-test-split":{"title":"Training-test split","content":"\nWe can develop and optimize a model on the training set\n\nTest the efficiency on the test set\n\n````r\nlibrary(rsample)\nset.seed(0) # for reproducibility\n\nauto_split \u003c- initial_split(df, prop = 0.8)\n\ndf_train \u003c- training(auto_split)\ndf_test \u003c- testing(auto_split)\n````\n\nWe should only use the test set once\n\nAvoid data leakage?\nAvoid still overfitting on the test data if we iterate and change our mdoela dn test it on test set each time\n\nThis is why we can use validation set:\n\ne.g.\n\n* Log transform variable\n* Add predictors\n\nThen we test the performance on the validation set before completing our model\n\nOnce our model is complete then we use the test set\n\n![Screenshot 2023-05-07 at 4.23.11 PM.png](Image%20Bank/Screenshot%202023-05-07%20at%204.23.11%20PM.png)\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Triple-Difference-in-Difference":{"title":"Triple Difference-in-Difference","content":"\nDDD\n\nTreated state\n\nPre vs post\n\nTreated individual\n\n## Regression\n\n$$ y\\_{it} = \\beta_0 + \\beta_1 Post_t + \\beta_2 S + \\beta_3 T + \\beta_4Post * S + \\beta_5 Post \\times T + \\beta_6S \\times T + \\beta_7 \\times Post \\times S \\times T + \\epsilon$$\n\n||Pre|Post|\n|--|---|----|\n|Treated state, treated group|$\\beta_0 + \\beta_2 + \\beta_3 + \\beta_6$|$\\beta_0 + \\beta_1 + \\beta_2 + \\beta_3 + \\beta_4 + \\beta_5 + \\beta_6 + \\beta_7$|\n|Treated state, untreated group|$\\beta_0 + \\beta_2$|$\\beta_0 + \\beta_1 + \\beta_2 + \\beta_4$|\n|After cancelling out|b0 + b2|b0 + b1 + b2 + b4|\n|Difference 1|**b5 + b7**||\n||||\n|Untreated state, treated group|b0 + b3|b0 + b1 + b3 + b5|\n|Untreated state, untreated group|b0|b0 + b1|\n|After cancelling out|b3|b3 + b5|\n|Difference 2|**b5**||\n|Ending Difference (1-2)|b7||\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Type-1-Error":{"title":"Type 1 Error","content":"\nDiscover non-sense\n\nFalse positive\n\nFail to reject the null hypothesis when it should be rejected?\n\nAccept alternative hyptohesis when it should not be accepted\n\nExplain simply type 1 error\n\nType 1 error is a statistical term that refers to a situation where a hypothesis is rejected when it is actually true. In other words, it is a false positive result. It can occur when the significance level of a test is set too high, or when the sample size is too small, leading to an incorrect conclusion that there is a significant difference between groups or variables when there isn't one. Type 1 error can have serious consequences in fields such as medicine and law, where incorrect decisions based on false positives can have negative impacts on individuals or society as a whole.\n\n![Pasted image 20230507153314.png](Image%20Bank/Pasted%20image%2020230507153314.png)\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Uncertainty":{"title":"Uncertainty","content":"\nA state of information\n\nExplain what uncertainty is in econometricsUncertainty in econometrics refers to the degree of doubt or unpredictability that exists in statistical or economic models. It arises from the fact that many economic phenomena are subject to various sources of randomness, such as measurement errors, unobserved variables, and unpredictable shocks.\n\nUncertainty can affect the accuracy and reliability of econometric models, as it makes it difficult to estimate the true values of parameters and to make reliable forecasts. Moreover, uncertainty can also have important implications for policy decisions, as it may lead to different outcomes depending on how policymakers interpret and respond to uncertain information.\n\nTo deal with uncertainty in econometrics, researchers use various techniques such as sensitivity analysis, hypothesis testing, and Monte Carlo simulations. These tools allow them to assess the robustness of their models under different scenarios and to quantify the uncertainty associated with their estimates. \n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Unconditional-Mean":{"title":"Unconditional Mean","content":"\nNot depending on other variables/regressors?\n\n$$\\Large \\mu X = \\mathbb{E}\\[X]$$\nX = e.g. spending in America\n\nThus the mean of spending in America = the expectation\nYou can estimate this with the sample average\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Validation-Set":{"title":"Validation Set","content":"\nUsed for model evaluation when still in the training phase to avoid having to use the test set for model training iteration (e.g. log transformations, adding predictors etc.)\n\nIn [Cross-Validation](Cross-Validation.md) it can also be called a fold or assessment set\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Variance":{"title":"Variance","content":"\nThis is [Standard Deviation](Standard%20Deviation.md) squared\n\nThe variance of a [Random Variable](Random%20Variable.md) is the weighted average of the squared distances between each value x and the expectation of X.\n\nExpectated value of the squared deviation of the mean\n\n$$Var(X) = E\\[(X - E\\[X])^2]$$\n\n![variance-and-standard-deviation-4-1651274333.jpg 1.png](Image%20Bank/variance-and-standard-deviation-4-1651274333.jpg%201.png)\n\n![Variance-and-Standard-Deviation 1.png](Image%20Bank/Variance-and-Standard-Deviation%201.png)\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Vector":{"title":"Vector","content":"\nExplain simply what vectors are and how they are using in data science and in programming languages like R\n\nVectors are mathematical objects that can be used to represent and manipulate multiple values or variables at once. In data science, vectors are often used to store and analyze large sets of data, such as survey responses or sensor readings. In programming languages like R, vectors can be created using functions like c() and used to perform operations like addition, multiplication, and indexing. Vectors are an essential tool for data manipulation and analysis in many fields, including statistics, machine learning, and data visualization.\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/Welfare-and-Labor-Supply":{"title":"Welfare and Labor Supply","content":"\n\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/relative-poverty":{"title":"relative poverty","content":"\nOne measure: INcomes shares by percentile\n\n[Gini Coefficient](Gini%20Coefficient.md): considers entire income distribution\n\nGraphically, Gini = A/(A+B)\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/t-test":{"title":"t-test","content":"\n\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]},"/notes/z-value":{"title":"z-value","content":"\nThe z-value is a measure of how many standard deviations a test statistic (e.g., the estimated coefficient divided by its standard error in logistic regression) is away from its expected value under the null hypothesis. It is calculated by subtracting the null hypothesis value from the observed value and dividing by the standard error. The z-value is used to calculate the p-value, which represents the probability of observing a test statistic as extreme as the one calculated, assuming the null hypothesis is true.\n","lastmodified":"2023-05-25T06:37:17.3866556Z","tags":[]}}