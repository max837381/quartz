---
---

Entropy is a measure of uncertainty in a probability distribution. It measures the amount of randomness in a system and is closely related to the concept of information. The entropy of a probability distribution is the expected value of the information contained in a random variable. In other words, it's a measure of how much information can be gained from the distribution. It can be thought of as a measure of how unpredictable a system is. For example, a coin toss has an entropy of 1 bit because there is only two possible outcomes (heads or tails). On the other hand, a dice roll has an entropy of 2.58 bits because there are six possible outcomes. Entropy can also be used to measure the amount of uncertainty in a system by looking at how evenly distributed the probabilities are. The more evenly distributed the probabilities, the higher the entropy.
